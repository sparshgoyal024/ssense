{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-commerce Fraud Detection using SageMaker\n",
    "\n",
    "This notebook demonstrates how to build a machine learning model to detect fraudulent e-commerce transactions. We'll use SageMaker to train and deploy the model, then test it on simulated transaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import io\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('./src/'))\n",
    "from package import config\n",
    "\n",
    "# Initialize AWS clients\n",
    "session = sagemaker.Session()\n",
    "s3 = boto3.resource('s3')\n",
    "sm_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Enhance the existing data generation with more fraud patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def load_transaction_data_from_s3(bucket_name=None, prefix='historical-data', filename='transactions.json'):\n",
    "    \"\"\"\n",
    "    Load transaction data from S3 bucket instead of generating it\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    bucket_name : str, optional\n",
    "        The S3 bucket name. If None, will try to get from environment variable.\n",
    "    prefix : str, optional\n",
    "        The prefix in the S3 bucket where the data is stored.\n",
    "    filename : str, optional\n",
    "        The filename of the transaction data.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: DataFrame containing the transaction data\n",
    "    \"\"\"\n",
    "    # Get bucket name from environment variable if not provided\n",
    "    if bucket_name is None:\n",
    "        bucket_name = os.environ.get('MODEL_DATA_S3_BUCKET', 'ssense-fraud-model-data')\n",
    "    \n",
    "    # Initialize S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Full S3 key\n",
    "    s3_key = f\"{prefix}/{filename}\"\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Loading transaction data from s3://{bucket_name}/{s3_key}\")\n",
    "        \n",
    "        # Get object from S3\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "        \n",
    "        # Read JSON content\n",
    "        json_content = response['Body'].read().decode('utf-8')\n",
    "        transactions = json.loads(json_content)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(transactions)\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {len(df)} transactions from S3\")\n",
    "        logger.info(f\"Fraud percentage: {df['is_fraud'].mean() * 100:.2f}%\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading transaction data from S3: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "df = load_transaction_data_from_s3()\n",
    "df.head()\n",
    "\n",
    "# To use in a SageMaker notebook:\n",
    "def load_data_for_training(bucket_name=None):\n",
    "    \"\"\"\n",
    "    Load transaction data from S3 and prepare it for model training\n",
    "    \"\"\"\n",
    "    # Load data from S3\n",
    "    df = load_transaction_data_from_s3(bucket_name)\n",
    "    \n",
    "    # Display info about the loaded data\n",
    "    print(f\"Loaded {len(df)} transactions from S3\")\n",
    "    print(f\"Fraud percentage: {df['is_fraud'].mean() * 100:.2f}%\")\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return df\n",
    "\n",
    "# def generate_enhanced_transaction_data(num_transactions=10000, fraud_ratio=0.1):\n",
    "#     \"\"\"Generate enhanced transaction data with more sophisticated fraud patterns\"\"\"\n",
    "#     from package.generator import generate_transaction\n",
    "    \n",
    "#     transactions = []\n",
    "    \n",
    "#     # Generate legitimate transactions\n",
    "#     for _ in range(int(num_transactions * (1 - fraud_ratio))):\n",
    "#         transactions.append(generate_transaction(fraud_probability=0))\n",
    "    \n",
    "#     # Generate fraudulent transactions with specific patterns\n",
    "#     for _ in range(int(num_transactions * fraud_ratio)):\n",
    "#         transaction = generate_transaction(fraud_probability=1)\n",
    "        \n",
    "#         # Add more sophisticated fraud patterns\n",
    "#         if np.random.random() < 0.3:\n",
    "#             # Pattern 1: High value transactions from unusual locations using VPN\n",
    "#             transaction['amount'] = float(np.random.uniform(1000, 5000))\n",
    "#             transaction['location'] = np.random.choice(['Tokyo, Japan', 'Berlin, Germany', 'Sydney, Australia'])\n",
    "#             transaction['is_vpn'] = True\n",
    "#         elif np.random.random() < 0.5:\n",
    "#             # Pattern 2: Multiple small transactions using gift cards\n",
    "#             transaction['amount'] = float(np.random.uniform(50, 200))\n",
    "#             transaction['card_type'] = 'gift'\n",
    "#             transaction['device_type'] = 'mobile'\n",
    "#         else:\n",
    "#             # Pattern 3: Declined transactions attempted again\n",
    "#             transaction['status'] = 'declined'\n",
    "#             transaction['is_vpn'] = np.random.choice([True, False], p=[0.7, 0.3])\n",
    "        \n",
    "#         transactions.append(transaction)\n",
    "    \n",
    "#     # Convert to DataFrame\n",
    "#     df = pd.DataFrame(transactions)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Generate transaction data with enhanced fraud patterns\n",
    "# num_transactions = 20000  # Increase dataset size\n",
    "# fraud_ratio = 0.1        # 10% fraud rate for better training\n",
    "# df = generate_enhanced_transaction_data(num_transactions, fraud_ratio)\n",
    "\n",
    "# # Display sample data\n",
    "# print(f\"Generated {len(df)} transactions\")\n",
    "# print(f\"Fraud percentage: {df['is_vpn'].mean() * 100:.2f}%\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Feature Engineering\n",
    "\n",
    "Add more sophisticated feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def engineer_features_for_ml(df):\n",
    "    \"\"\"\n",
    "    Feature engineering for ML-based fraud detection.\n",
    "    Uses the ground truth fraud labels from the data rather than creating rule-based labels.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Ensure we have a fraud label column from the data\n",
    "    if 'is_fraud' not in df_features.columns:\n",
    "        raise ValueError(\"Data must contain an 'is_fraud' column with ground truth labels\")\n",
    "    \n",
    "    # Convert timestamp to datetime if it's not already\n",
    "    if not pd.api.types.is_datetime64_dtype(df_features['timestamp']):\n",
    "        df_features['timestamp'] = pd.to_datetime(df_features['timestamp'])\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df_features['hour_of_day'] = df_features['timestamp'].dt.hour\n",
    "    df_features['day_of_week'] = df_features['timestamp'].dt.dayofweek\n",
    "    df_features['month'] = df_features['timestamp'].dt.month\n",
    "    df_features['day_of_month'] = df_features['timestamp'].dt.day\n",
    "    df_features['is_weekend'] = df_features['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df_features['is_night'] = df_features['hour_of_day'].apply(lambda x: 1 if (x < 6 or x >= 22) else 0)\n",
    "    \n",
    "    # User behavior features - calculated properly for both training and prediction\n",
    "    # Group by user_id to get transaction counts and statistics\n",
    "    user_stats = df_features.groupby('user_id').agg({\n",
    "        'transaction_id': 'count',\n",
    "        'amount': ['mean', 'std', 'min', 'max'],\n",
    "        'is_vpn': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Flatten multi-level columns\n",
    "    user_stats.columns = ['_'.join(col).strip() for col in user_stats.columns.values]\n",
    "    user_stats.rename(columns={\n",
    "        'transaction_id_count': 'user_transaction_count',\n",
    "        'amount_mean': 'user_avg_amount',\n",
    "        'amount_std': 'user_amount_std',\n",
    "        'amount_min': 'user_min_amount',\n",
    "        'amount_max': 'user_max_amount',\n",
    "        'is_vpn_mean': 'user_vpn_ratio'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Handle users with only one transaction (no std)\n",
    "    user_stats['user_amount_std'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Reset index to make user_id a column again\n",
    "    user_stats.reset_index(inplace=True)\n",
    "    \n",
    "    # Merge user statistics back to the main dataframe\n",
    "    df_features = df_features.merge(user_stats, on='user_id', how='left')\n",
    "    \n",
    "    # Calculate transaction amount z-score relative to user's history\n",
    "    # Use a safe calculation that handles division by zero\n",
    "    df_features['amount_zscore'] = df_features.apply(\n",
    "        lambda row: (row['amount'] - row['user_avg_amount']) / (row['user_amount_std'] if row['user_amount_std'] > 0 else 1),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create features for relative transaction size\n",
    "    df_features['amount_to_max_ratio'] = df_features.apply(\n",
    "        lambda row: row['amount'] / row['user_max_amount'] if row['user_max_amount'] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Location features - use past fraud rates instead of hardcoded risk\n",
    "    location_fraud_rates = df_features.groupby('location')['is_fraud'].mean().to_dict()\n",
    "    df_features['location_fraud_rate'] = df_features['location'].map(location_fraud_rates)\n",
    "    \n",
    "    # Handle NaN values for new locations\n",
    "    df_features['location_fraud_rate'].fillna(df_features['is_fraud'].mean(), inplace=True)\n",
    "    \n",
    "    # Device and card type features\n",
    "    device_fraud_rates = df_features.groupby('device_type')['is_fraud'].mean().to_dict()\n",
    "    df_features['device_fraud_rate'] = df_features['device_type'].map(device_fraud_rates)\n",
    "    \n",
    "    card_fraud_rates = df_features.groupby('card_type')['is_fraud'].mean().to_dict()\n",
    "    df_features['card_fraud_rate'] = df_features['card_type'].map(card_fraud_rates)\n",
    "    \n",
    "    # Convert boolean to integer if needed\n",
    "    if pd.api.types.is_bool_dtype(df_features['is_vpn']):\n",
    "        df_features['is_vpn'] = df_features['is_vpn'].astype(int)\n",
    "    \n",
    "    # Create dummy variables for categorical features\n",
    "    categorical_features = ['device_type', 'card_type', 'status', 'location']\n",
    "    df_features = pd.get_dummies(df_features, columns=categorical_features, drop_first=False)\n",
    "    \n",
    "    # New feature: is this amount unusual for this user?\n",
    "    df_features['is_unusual_amount'] = (\n",
    "        (df_features['amount'] > (df_features['user_avg_amount'] + 2 * df_features['user_amount_std'])) |\n",
    "        (df_features['amount'] < (df_features['user_avg_amount'] - 2 * df_features['user_amount_std']))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # New feature: is this a new user (fewer than N transactions)?\n",
    "    df_features['is_new_user'] = (df_features['user_transaction_count'] <= 3).astype(int)\n",
    "    \n",
    "    # New feature: is this the user's largest transaction?\n",
    "    df_features['is_largest_tx'] = (df_features['amount'] >= df_features['user_max_amount'] * 0.95).astype(int)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "def create_ml_preprocessing_pipeline(df_features):\n",
    "    \"\"\"\n",
    "    Create a scikit-learn preprocessing pipeline for ML model training.\n",
    "    This ensures consistent feature transformation for training and prediction.\n",
    "    \"\"\"\n",
    "    # List your columns by type\n",
    "    numeric_features = [\n",
    "        'amount', 'hour_of_day', 'day_of_week', 'month', 'day_of_month', \n",
    "        'is_weekend', 'is_night', 'is_vpn', 'user_transaction_count',\n",
    "        'user_avg_amount', 'user_amount_std', 'user_min_amount', 'user_max_amount',\n",
    "        'user_vpn_ratio', 'amount_zscore', 'amount_to_max_ratio', \n",
    "        'location_fraud_rate', 'device_fraud_rate', 'card_fraud_rate',\n",
    "        'is_unusual_amount', 'is_new_user', 'is_largest_tx'\n",
    "    ]\n",
    "    \n",
    "    # Create column transformer for preprocessing\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "        ],\n",
    "        remainder='passthrough'  # This keeps the dummy variables without scaling\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def train_test_split_with_features(df):\n",
    "    \"\"\"\n",
    "    Load data, engineer features, and prepare train/test splits with proper feature preprocessing\n",
    "    \"\"\"\n",
    "    # Apply feature engineering\n",
    "    df_features = engineer_features_for_ml(df)\n",
    "    \n",
    "    # Display feature correlations with fraud\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    # Calculate correlation only for numeric columns\n",
    "    numeric_df = df_features.select_dtypes(include=['number'])\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "    fraud_correlations = correlation_matrix['is_fraud'].sort_values(ascending=False)\n",
    "    print(\"Top features correlated with fraud:\")\n",
    "    print(fraud_correlations.head(15))\n",
    "\n",
    "    # Plot correlation heatmap for top correlated features\n",
    "    top_corr_features = fraud_correlations.index[:15]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        correlation_matrix.loc[top_corr_features, top_corr_features], \n",
    "        annot=True, \n",
    "        cmap='coolwarm'\n",
    "    )\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the engineered features dataframe\n",
    "    return df_features\n",
    "# from load_data_utils import load_data_for_training\n",
    "\n",
    "# def engineer_features(df):\n",
    "#     \"\"\"More sophisticated feature engineering for fraud detection\"\"\"\n",
    "    \n",
    "#     # Create a copy to avoid modifying the original\n",
    "#     df_features = df.copy()\n",
    "    \n",
    "#     # Extract time-based features\n",
    "#     df_features['timestamp'] = pd.to_datetime(df_features['timestamp'])\n",
    "#     df_features['hour_of_day'] = df_features['timestamp'].dt.hour\n",
    "#     df_features['day_of_week'] = df_features['timestamp'].dt.dayofweek\n",
    "#     df_features['is_weekend'] = df_features['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "#     df_features['is_night'] = df_features['hour_of_day'].apply(lambda x: 1 if (x < 6 or x >= 22) else 0)\n",
    "    \n",
    "#     # Location risk score\n",
    "#     location_risk = {\n",
    "#         'California, USA': 0.2,\n",
    "#         'New York, USA': 0.2,\n",
    "#         'Texas, USA': 0.2,\n",
    "#         'Florida, USA': 0.3,\n",
    "#         'Illinois, USA': 0.2,\n",
    "#         'London, UK': 0.3,\n",
    "#         'Paris, France': 0.4,\n",
    "#         'Berlin, Germany': 0.4,\n",
    "#         'Tokyo, Japan': 0.5,\n",
    "#         'Sydney, Australia': 0.5,\n",
    "#         'Unknown': 0.9\n",
    "#     }\n",
    "#     df_features['location_risk'] = df_features['location'].map(location_risk)\n",
    "    \n",
    "#     # User behavior features\n",
    "#     # Group by user ID to analyze user patterns\n",
    "#     user_transaction_counts = df_features.groupby('user_id').size().reset_index(name='user_transaction_count')\n",
    "#     df_features = df_features.merge(user_transaction_counts, on='user_id', how='left')\n",
    "    \n",
    "#     user_amount_avg = df_features.groupby('user_id')['amount'].mean().reset_index(name='user_avg_amount')\n",
    "#     df_features = df_features.merge(user_amount_avg, on='user_id', how='left')\n",
    "    \n",
    "#     # Transaction amount z-score (compared to user's average)\n",
    "#     df_features['amount_zscore'] = df_features.apply(\n",
    "#         lambda row: (row['amount'] - row['user_avg_amount']) / df_features['amount'].std() \n",
    "#         if row['user_transaction_count'] > 1 else 0, axis=1\n",
    "#     )\n",
    "    \n",
    "#     # Device-location mismatch (unusual device for location)\n",
    "#     df_features['device_type_num'] = df_features['device_type'].map({\n",
    "#         'mobile': 0, 'desktop': 1, 'tablet': 2\n",
    "#     })\n",
    "    \n",
    "#     # Create dummies for categorical variables\n",
    "#     df_features = pd.get_dummies(\n",
    "#         df_features, \n",
    "#         columns=['device_type', 'card_type', 'status'], \n",
    "#         drop_first=False\n",
    "#     )\n",
    "    \n",
    "#     # Convert boolean to integer\n",
    "#     df_features['is_vpn'] = df_features['is_vpn'].astype(int)\n",
    "    \n",
    "#     # Define a fraud label - this depends on how fraud is defined in your synthetic data\n",
    "#     # In this case, we'll use is_vpn + high amount + unusual location as indicators\n",
    "#     if 'is_fraud' not in df_features.columns:\n",
    "#         df_features['is_fraud'] = ((df_features['is_vpn'] == 1) & \n",
    "#                                   (df_features['amount'] > 500) & \n",
    "#                                   (df_features['location_risk'] > 0.3)).astype(int)\n",
    "    \n",
    "#     # Compute additional risk factors\n",
    "#     df_features['transaction_risk_score'] = (\n",
    "#         df_features['amount_zscore'] * 0.3 +\n",
    "#         df_features['location_risk'] * 0.2 +\n",
    "#         df_features['is_vpn'] * 0.2 +\n",
    "#         df_features['is_night'] * 0.1\n",
    "#     )\n",
    "    \n",
    "#     # Add status_declined if it exists\n",
    "#     if 'status_declined' in df_features.columns:\n",
    "#         df_features['transaction_risk_score'] += df_features['status_declined'] * 0.2\n",
    "    \n",
    "#     return df_features\n",
    "\n",
    "# # Apply feature engineering\n",
    "# df_engineered = engineer_features(df)\n",
    "\n",
    "# # Display feature correlations with fraud\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# correlation_matrix = df_engineered.corr()\n",
    "# fraud_correlations = correlation_matrix['is_fraud'].sort_values(ascending=False)\n",
    "# print(\"Top features correlated with fraud:\")\n",
    "# print(fraud_correlations.head(10))\n",
    "\n",
    "# # Plot correlation heatmap\n",
    "# sns.heatmap(correlation_matrix.iloc[:15, :15], annot=True, cmap='coolwarm')\n",
    "# plt.title('Feature Correlation Matrix')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data from S3\n",
    "from load_data_utils import load_data_for_training\n",
    "df = load_data_for_training()\n",
    "\n",
    "# Apply the improved feature engineering\n",
    "from feature_engineering import engineer_features_for_ml, create_ml_preprocessing_pipeline\n",
    "df_engineered = engineer_features_for_ml(df)\n",
    "\n",
    "# Define numeric features to use for the model\n",
    "numeric_features = [\n",
    "    'amount', 'hour_of_day', 'day_of_week', 'month', 'day_of_month',\n",
    "    'is_weekend', 'is_night', 'is_vpn', 'user_transaction_count',\n",
    "    'user_avg_amount', 'user_amount_std', 'amount_zscore', \n",
    "    'amount_to_max_ratio', 'location_fraud_rate', 'device_fraud_rate', \n",
    "    'card_fraud_rate', 'is_unusual_amount', 'is_new_user', 'is_largest_tx'\n",
    "]\n",
    "\n",
    "# Get all one-hot encoded categorical columns\n",
    "categorical_columns = [col for col in df_engineered.columns if \n",
    "                      col.startswith('device_type_') or \n",
    "                      col.startswith('card_type_') or \n",
    "                      col.startswith('status_') or\n",
    "                      col.startswith('location_')]\n",
    "\n",
    "# Combine all features\n",
    "features = numeric_features + categorical_columns\n",
    "\n",
    "# Define target\n",
    "target = 'is_fraud'\n",
    "\n",
    "# Check if all features exist in the dataframe\n",
    "missing_features = [f for f in features if f not in df_engineered.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: These features are missing from the dataframe: {missing_features}\")\n",
    "    # Keep only features that exist in the dataframe\n",
    "    features = [f for f in features if f in df_engineered.columns]\n",
    "\n",
    "# Split the data\n",
    "X = df_engineered[features]\n",
    "y = df_engineered[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(f\"Fraud ratio in training: {y_train.mean():.2f}\")\n",
    "print(f\"Fraud ratio in testing: {y_test.mean():.2f}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = create_ml_preprocessing_pipeline(df_engineered)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed training data shape: {X_train_processed.shape}\")\n",
    "\n",
    "# Look at the distribution of numeric features after preprocessing\n",
    "X_train_processed_df = pd.DataFrame(\n",
    "    X_train_processed[:, :len(numeric_features)],\n",
    "    columns=numeric_features\n",
    ")\n",
    "\n",
    "print(\"\\nProcessed Feature Statistics:\")\n",
    "print(X_train_processed_df.describe().transpose()[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Identify the most important features using correlation with the target\n",
    "top_correlations = X_train.corrwith(y_train).abs().sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features correlated with fraud:\")\n",
    "print(top_correlations.head(10))\n",
    "\n",
    "# # Select features for model training\n",
    "# features = ['amount', 'hour_of_day', 'day_of_week', 'is_weekend', 'is_night',\n",
    "#             'location_risk', 'is_vpn', 'user_transaction_count', 'amount_zscore',\n",
    "#             'transaction_risk_score']\n",
    "\n",
    "# # Add categorical columns (from one-hot encoding)\n",
    "# categorical_columns = [col for col in df_engineered.columns if \n",
    "#                       col.startswith('device_type_') or \n",
    "#                       col.startswith('card_type_') or \n",
    "#                       col.startswith('status_')]\n",
    "# features.extend(categorical_columns)\n",
    "\n",
    "# # Define target\n",
    "# target = 'is_fraud'\n",
    "\n",
    "# # Split the data\n",
    "# X = df_engineered[features]\n",
    "# y = df_engineered[target]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# print(f\"Training set shape: {X_train.shape}\")\n",
    "# print(f\"Testing set shape: {X_test.shape}\")\n",
    "# print(f\"Fraud ratio in training: {y_train.mean():.2f}\")\n",
    "# print(f\"Fraud ratio in testing: {y_test.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import os\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "def upload_training_data_to_s3(X_train, y_train, X_val=None, y_val=None):\n",
    "    \"\"\"\n",
    "    Convert training data to SVM light format and upload to S3\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas.DataFrame\n",
    "        Training features\n",
    "    y_train : pandas.Series\n",
    "        Training labels\n",
    "    X_val : pandas.DataFrame, optional\n",
    "        Validation features\n",
    "    y_val : pandas.Series, optional\n",
    "        Validation labels\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary containing S3 URIs for training and validation data\n",
    "    \"\"\"\n",
    "    # Get S3 bucket from config\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = os.environ.get('MODEL_DATA_S3_BUCKET', 'ssense-fraud-model-data')\n",
    "    prefix = 'fraud-classifier'\n",
    "    \n",
    "    # Create a buffer for the training data in SVM light format\n",
    "    train_file = io.BytesIO()\n",
    "    dump_svmlight_file(X_train, y_train, train_file)\n",
    "    train_file.seek(0)\n",
    "    \n",
    "    # Upload training data to S3\n",
    "    train_key = f'{prefix}/train/train.libsvm'\n",
    "    s3.Bucket(bucket).Object(train_key).upload_fileobj(train_file)\n",
    "    train_data_s3_uri = f's3://{bucket}/{train_key}'\n",
    "    print(f\"Uploaded training data to {train_data_s3_uri}\")\n",
    "    \n",
    "    # If validation data is provided, upload it too\n",
    "    val_data_s3_uri = None\n",
    "    if X_val is not None and y_val is not None:\n",
    "        val_file = io.BytesIO()\n",
    "        dump_svmlight_file(X_val, y_val, val_file)\n",
    "        val_file.seek(0)\n",
    "        \n",
    "        val_key = f'{prefix}/validation/validation.libsvm'\n",
    "        s3.Bucket(bucket).Object(val_key).upload_fileobj(val_file)\n",
    "        val_data_s3_uri = f's3://{bucket}/{val_key}'\n",
    "        print(f\"Uploaded validation data to {val_data_s3_uri}\")\n",
    "    \n",
    "    # Set output location\n",
    "    output_s3_uri = f's3://{bucket}/{prefix}/output'\n",
    "    \n",
    "    return {\n",
    "        'train_data_s3_uri': train_data_s3_uri,\n",
    "        'validation_data_s3_uri': val_data_s3_uri,\n",
    "        'output_s3_uri': output_s3_uri\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# s3_uris = upload_training_data_to_s3(X_train, y_train, X_val, y_val)\n",
    "# train_data_s3_uri = s3_uris['train_data_s3_uri']\n",
    "# output_s3_uri = s3_uris['output_s3_uri']\n",
    "\n",
    "#  Train using SageMaker built-in XGBoost algorithm\n",
    "# First, prepare data in SVM light format required by SageMaker XGBoost\n",
    "# train_file = io.BytesIO()\n",
    "# dump_svmlight_file(X_train, y_train, train_file)\n",
    "# train_file.seek(0)\n",
    "\n",
    "# # Upload data to S3\n",
    "# bucket = config.MODEL_DATA_S3_BUCKET\n",
    "# prefix = 'fraud-classifier'\n",
    "# key = f'{prefix}/train/train.libsvm'\n",
    "\n",
    "# s3.Bucket(bucket).Object(key).upload_fileobj(train_file)\n",
    "# train_data_s3_uri = f's3://{bucket}/{key}'\n",
    "# print(f\"Uploaded training data to {train_data_s3_uri}\")\n",
    "\n",
    "# # Set output location\n",
    "\n",
    "# output_s3_uri = f's3://{bucket}/{prefix}/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a further split of training data to get a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create train file in SVMlight format\n",
    "train_file = io.BytesIO()\n",
    "dump_svmlight_file(X_train, y_train, train_file)\n",
    "train_file.seek(0)\n",
    "\n",
    "# Create validation file in SVMlight format\n",
    "validation_file = io.BytesIO()\n",
    "dump_svmlight_file(X_val, y_val, validation_file)\n",
    "validation_file.seek(0)\n",
    "\n",
    "# Upload data to S3\n",
    "bucket = MODEL_DATA_S3_BUCKET\n",
    "prefix = 'fraud-classifier'\n",
    "\n",
    "# Upload train data\n",
    "train_key = f'{prefix}/train/train.libsvm'\n",
    "s3.Bucket(bucket).Object(train_key).upload_fileobj(train_file)\n",
    "train_data_s3_uri = f's3://{bucket}/{train_key}'\n",
    "\n",
    "# Upload validation data\n",
    "val_key = f'{prefix}/validation/validation.libsvm'\n",
    "s3.Bucket(bucket).Object(val_key).upload_fileobj(validation_file)\n",
    "validation_data_s3_uri = f's3://{bucket}/{val_key}'\n",
    "\n",
    "# Set output location\n",
    "output_s3_uri = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "print(f\"Uploaded training data to {train_data_s3_uri}\")\n",
    "print(f\"Uploaded validation data to {validation_data_s3_uri}\")\n",
    "\n",
    "# Get the XGBoost image - use the newer SageMaker API\n",
    "container = image_uris.retrieve(\"xgboost\", boto3.Session().region_name, version=\"1.0-1\")\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.01, 0.3),\n",
    "    'gamma': ContinuousParameter(0, 5),\n",
    "    'min_child_weight': IntegerParameter(1, 10),\n",
    "    'subsample': ContinuousParameter(0.5, 1.0),\n",
    "    'colsample_bytree': ContinuousParameter(0.5, 1.0)\n",
    "}\n",
    "\n",
    "# Create an estimator with both train and validation channels\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=SAGEMAKER_IAM_ROLE,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    output_path=output_s3_uri,\n",
    "    sagemaker_session=session,\n",
    "    base_job_name='fraud-detection-xgb'\n",
    ")\n",
    "\n",
    "# Set static hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    num_round=100,\n",
    "    rate_drop=0.1,\n",
    "    scale_pos_weight=10,  # Helpful for imbalanced datasets\n",
    "    # Add the following for better handling of missing values and numerical stability\n",
    "    tree_method='auto',\n",
    "    max_delta_step=3,    # Helpful for unbalanced classes\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Create the tuner with the correct metric name\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb,\n",
    "    'validation:auc',  # Make sure this matches eval_metric\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=5,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type='Maximize'\n",
    ")\n",
    "\n",
    "# Start the hyperparameter tuning job with both train and validation\n",
    "tuner.fit({\n",
    "    'train': train_data_s3_uri,\n",
    "    'validation': validation_data_s3_uri\n",
    "})\n",
    "print(\"Hyperparameter tuning job started\")\n",
    "\n",
    "# Save feature information for later use with the model\n",
    "feature_info = {\n",
    "    'feature_names': list(X_train.columns),\n",
    "    'categorical_features': [col for col in X_train.columns if \n",
    "                             col.startswith('device_type_') or \n",
    "                             col.startswith('card_type_') or \n",
    "                             col.startswith('status_') or\n",
    "                             col.startswith('location_')],\n",
    "    'numeric_features': [col for col in X_train.columns if \n",
    "                         not (col.startswith('device_type_') or \n",
    "                              col.startswith('card_type_') or \n",
    "                              col.startswith('status_') or\n",
    "                              col.startswith('location_'))]\n",
    "}\n",
    "\n",
    "# Store feature information in S3 for deployment\n",
    "feature_info_key = f'{prefix}/model/feature_info.json'\n",
    "s3.Bucket(bucket).Object(feature_info_key).put(\n",
    "    Body=json.dumps(feature_info, indent=2)\n",
    ")\n",
    "print(f\"Saved feature information to s3://{bucket}/{feature_info_key}\")\n",
    "\n",
    "# Prepare data in SVM light format required by SageMaker XGBoost\n",
    "# First properly split into train and validation\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create train file\n",
    "# train_file = io.BytesIO()\n",
    "# dump_svmlight_file(X_train, y_train, train_file)\n",
    "# train_file.seek(0)\n",
    "\n",
    "# # Create validation file\n",
    "# validation_file = io.BytesIO()\n",
    "# dump_svmlight_file(X_val, y_val, validation_file)\n",
    "# validation_file.seek(0)\n",
    "\n",
    "# # Upload data to S3\n",
    "# bucket = MODEL_DATA_S3_BUCKET\n",
    "# prefix = 'fraud-classifier'\n",
    "\n",
    "# # Upload train data\n",
    "# train_key = f'{prefix}/train/train.libsvm'\n",
    "# s3.Bucket(bucket).Object(train_key).upload_fileobj(train_file)\n",
    "# train_data_s3_uri = f's3://{bucket}/{train_key}'\n",
    "\n",
    "# # Upload validation data\n",
    "# val_key = f'{prefix}/validation/validation.libsvm'\n",
    "# s3.Bucket(bucket).Object(val_key).upload_fileobj(validation_file)\n",
    "# validation_data_s3_uri = f's3://{bucket}/{val_key}'\n",
    "\n",
    "# # Set output location\n",
    "# output_s3_uri = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "# print(f\"Uploaded training data to {train_data_s3_uri}\")\n",
    "# print(f\"Uploaded validation data to {validation_data_s3_uri}\")\n",
    "\n",
    "# # Get the XGBoost image - use the newer SageMaker API\n",
    "# from sagemaker import image_uris\n",
    "# container = image_uris.retrieve(\"xgboost\", boto3.Session().region_name, version=\"1.0-1\")\n",
    "\n",
    "# # Set up hyperparameter tuning job\n",
    "# from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# # Define hyperparameter ranges\n",
    "# hyperparameter_ranges = {\n",
    "#     'max_depth': IntegerParameter(3, 10),\n",
    "#     'eta': ContinuousParameter(0.01, 0.3),\n",
    "#     'gamma': ContinuousParameter(0, 5),\n",
    "#     'min_child_weight': IntegerParameter(1, 10),\n",
    "#     'subsample': ContinuousParameter(0.5, 1.0),\n",
    "#     'colsample_bytree': ContinuousParameter(0.5, 1.0)\n",
    "# }\n",
    "\n",
    "# # Create an estimator with both train and validation channels\n",
    "# xgb = sagemaker.estimator.Estimator(\n",
    "#     container,\n",
    "#     role=SAGEMAKER_IAM_ROLE,\n",
    "#     train_instance_count=1,\n",
    "#     train_instance_type='ml.m5.xlarge',\n",
    "#     output_path=output_s3_uri,\n",
    "#     sagemaker_session=session,\n",
    "#     base_job_name='fraud-detection-xgb'\n",
    "# )\n",
    "\n",
    "# # Set static hyperparameters\n",
    "# xgb.set_hyperparameters(\n",
    "#     objective='binary:logistic',\n",
    "#     eval_metric='auc',\n",
    "#     num_round=100,\n",
    "#     rate_drop=0.1,\n",
    "#     scale_pos_weight=10  # Helpful for imbalanced datasets\n",
    "# )\n",
    "\n",
    "# # Create the tuner with the correct metric name\n",
    "# tuner = HyperparameterTuner(\n",
    "#     xgb,\n",
    "#     'validation:auc',  # Make sure this matches eval_metric\n",
    "#     hyperparameter_ranges,\n",
    "#     max_jobs=5,\n",
    "#     max_parallel_jobs=2,\n",
    "#     objective_type='Maximize'\n",
    "# )\n",
    "\n",
    "# # Start the hyperparameter tuning job with both train and validation\n",
    "# tuner.fit({\n",
    "#     'train': train_data_s3_uri,\n",
    "#     'validation': validation_data_s3_uri\n",
    "# })\n",
    "# print(\"Hyperparameter tuning job started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize SageMaker client\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# Get the best model from hyperparameter tuning\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "best_job_name = sm_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")['BestTrainingJob']['TrainingJobName']\n",
    "\n",
    "print(f\"Best training job: {best_job_name}\")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hyperparameters = sm_client.describe_training_job(\n",
    "    TrainingJobName=best_job_name\n",
    ")['HyperParameters']\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "for param, value in best_hyperparameters.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Create model\n",
    "model_name = f\"fraud-detection-model-{int(time.time())}\"\n",
    "model_info = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': f\"{output_s3_uri}/{best_job_name}/output/model.tar.gz\"\n",
    "    },\n",
    "    ExecutionRoleArn=SAGEMAKER_IAM_ROLE\n",
    ")\n",
    "\n",
    "print(f\"Created model: {model_name}\")\n",
    "\n",
    "# Store model metadata including the feature information\n",
    "model_metadata = {\n",
    "    'model_name': model_name,\n",
    "    'training_job': best_job_name,\n",
    "    'hyperparameters': {k: v for k, v in best_hyperparameters.items() if not k.startswith('_')},\n",
    "    'creation_time': time.time(),\n",
    "    'feature_info': feature_info  # This comes from the previous step\n",
    "}\n",
    "\n",
    "# Save model metadata to S3\n",
    "metadata_key = f'{prefix}/models/{model_name}/metadata.json'\n",
    "s3.Bucket(bucket).Object(metadata_key).put(\n",
    "    Body=json.dumps(model_metadata, indent=2)\n",
    ")\n",
    "\n",
    "print(f\"Saved model metadata to s3://{bucket}/{metadata_key}\")\n",
    "\n",
    "# Get the best model from hyperparameter tuning\n",
    "# tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "# best_job_name = sm_client.describe_hyper_parameter_tuning_job(\n",
    "#     HyperParameterTuningJobName=tuning_job_name\n",
    "# )['BestTrainingJob']['TrainingJobName']\n",
    "\n",
    "# # Create model\n",
    "# model_name = f\"fraud-detection-model-{int(time.time())}\"\n",
    "# model_info = sm_client.create_model(\n",
    "#     ModelName=model_name,\n",
    "#     PrimaryContainer={\n",
    "#         'Image': container,\n",
    "#         'ModelDataUrl': f\"{output_s3_uri}/{best_job_name}/output/model.tar.gz\"\n",
    "#     },\n",
    "#     ExecutionRoleArn=config.SAGEMAKER_IAM_ROLE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Initialize SageMaker client\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# Create endpoint configuration with auto-scaling\n",
    "endpoint_config_name = f\"fraud-detection-config-{int(time.time())}\"\n",
    "endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'VariantName': 'default',\n",
    "        'ModelName': model_name,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InstanceType': 'ml.t2.medium',\n",
    "        'InitialVariantWeight': 1\n",
    "    }]\n",
    ")\n",
    "\n",
    "logger.info(f\"Created endpoint configuration: {endpoint_config_name}\")\n",
    "\n",
    "# Create endpoint\n",
    "endpoint_name = f\"{SOLUTION_PREFIX}-xgb\"\n",
    "endpoint = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "logger.info(f\"Endpoint {endpoint_name} creation initiated\")\n",
    "\n",
    "# Wait for endpoint to become available\n",
    "logger.info(\"Waiting for endpoint to be in service...\")\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "logger.info(f\"Endpoint {endpoint_name} is now in service\")\n",
    "\n",
    "# Store endpoint information in a central registry\n",
    "endpoint_info = {\n",
    "    'endpoint_name': endpoint_name,\n",
    "    'model_name': model_name,\n",
    "    'creation_time': time.time(),\n",
    "    'instance_type': 'ml.t2.medium',\n",
    "    'instance_count': 1,\n",
    "    'endpoint_config': endpoint_config_name\n",
    "}\n",
    "\n",
    "# Save endpoint info to S3\n",
    "endpoint_info_key = f'{prefix}/endpoints/{endpoint_name}/info.json'\n",
    "s3.Bucket(bucket).Object(endpoint_info_key).put(\n",
    "    Body=json.dumps(endpoint_info, indent=2)\n",
    ")\n",
    "\n",
    "print(f\"Endpoint {endpoint_name} is ready for inference\")\n",
    "\n",
    "\n",
    "\n",
    "# Create endpoint configuration with auto-scaling\n",
    "# endpoint_config_name = f\"fraud-detection-config-{int(time.time())}\"\n",
    "# endpoint_config = sm_client.create_endpoint_config(\n",
    "#     EndpointConfigName=endpoint_config_name,\n",
    "#     ProductionVariants=[{\n",
    "#         'VariantName': 'default',\n",
    "#         'ModelName': model_name,\n",
    "#         'InitialInstanceCount': 1,\n",
    "#         'InstanceType': 'ml.t2.medium',\n",
    "#         'InitialVariantWeight': 1\n",
    "#     }]\n",
    "# )\n",
    "\n",
    "# # Create endpoint\n",
    "# endpoint_name = f\"{config.SOLUTION_PREFIX}-xgb\"\n",
    "# endpoint = sm_client.create_endpoint(\n",
    "#     EndpointName=endpoint_name,\n",
    "#     EndpointConfigName=endpoint_config_name\n",
    "# )\n",
    "# print(f\"Endpoint {endpoint_name} creation initiated\")\n",
    "\n",
    "# # Wait for endpoint to become available\n",
    "# print(\"Waiting for endpoint to be in service...\")\n",
    "# waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "# waiter.wait(EndpointName=endpoint_name)\n",
    "# print(f\"Endpoint {endpoint_name} is now in service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Create a predictor for the endpoint\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=CSVSerializer()\n",
    ")\n",
    "\n",
    "# Test on the test set\n",
    "def format_features_for_prediction(row):\n",
    "    \"\"\"Format a row of features for prediction\"\"\"\n",
    "    features_list = []\n",
    "    for feature in features:\n",
    "        # Handle missing features\n",
    "        if feature in row:\n",
    "            features_list.append(str(row[feature]))\n",
    "        else:\n",
    "            features_list.append('0')  # Default value for missing features\n",
    "    return ','.join(features_list)\n",
    "\n",
    "# Get predictions for test data\n",
    "print(\"Getting predictions for test data...\")\n",
    "y_pred_proba = []\n",
    "batch_size = 100  # Process in batches to avoid throttling\n",
    "\n",
    "# Use tqdm for a progress bar\n",
    "for i in tqdm(range(0, len(X_test), batch_size), desc=\"Processing batches\"):\n",
    "    batch = X_test.iloc[i:i+batch_size]\n",
    "    batch_features = [format_features_for_prediction(row) for _, row in batch.iterrows()]\n",
    "    \n",
    "    # Send each row separately to avoid CSV parsing issues\n",
    "    batch_predictions = []\n",
    "    for features_str in batch_features:\n",
    "        try:\n",
    "            response = predictor.predict(features_str)\n",
    "            pred = float(response.decode('utf-8'))\n",
    "            batch_predictions.append(pred)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with prediction: {str(e)}\")\n",
    "            print(f\"Problematic features: {features_str[:100]}...\")  # Print first 100 chars\n",
    "            batch_predictions.append(0.5)  # Default fallback value\n",
    "    \n",
    "    y_pred_proba.extend(batch_predictions)\n",
    "    \n",
    "    # Small delay to avoid throttling\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred_proba]\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Save test results to S3\n",
    "test_results = {\n",
    "    'endpoint_name': endpoint_name,\n",
    "    'test_time': time.time(),\n",
    "    'metrics': {\n",
    "        'roc_auc': float(roc_auc),\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    },\n",
    "    'test_size': len(X_test),\n",
    "    'positive_rate': float(sum(y_pred) / len(y_pred)),\n",
    "    'true_positive_rate': float(sum(y_pred * y_test) / sum(y_test)) if sum(y_test) > 0 else 0\n",
    "}\n",
    "\n",
    "# Save test results to S3\n",
    "test_results_key = f'{prefix}/endpoints/{endpoint_name}/test_results.json'\n",
    "s3.Bucket(bucket).Object(test_results_key).put(\n",
    "    Body=json.dumps(test_results, indent=2, default=str)\n",
    ")\n",
    "print(f\"Saved test results to s3://{bucket}/{test_results_key}\")\n",
    "\n",
    "# Example of getting and analyzing predictions for a few fraud and non-fraud transactions\n",
    "def analyze_transactions(X_sample, y_sample, predictor):\n",
    "    \"\"\"Analyze predictions for a sample of transactions\"\"\"\n",
    "    results = []\n",
    "    for i, (idx, row) in enumerate(X_sample.iterrows()):\n",
    "        features_str = format_features_for_prediction(row)\n",
    "        try:\n",
    "            response = predictor.predict(features_str)\n",
    "            pred_probability = float(response.decode('utf-8'))\n",
    "            is_fraud_prediction = pred_probability >= 0.5\n",
    "            \n",
    "            # Find the true label\n",
    "            true_label = y_sample.iloc[i]\n",
    "            \n",
    "            # Determine if prediction was correct\n",
    "            correct = (is_fraud_prediction == true_label)\n",
    "            \n",
    "            # Create a result dictionary\n",
    "            result = {\n",
    "                'index': idx,\n",
    "                'prediction': is_fraud_prediction,\n",
    "                'probability': pred_probability,\n",
    "                'true_label': true_label,\n",
    "                'correct': correct,\n",
    "                # Include a few key features for analysis\n",
    "                'features': {\n",
    "                    'amount': row.get('amount', 'N/A'),\n",
    "                    'is_vpn': row.get('is_vpn', 'N/A'),\n",
    "                    # Add any other key features you want to examine\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing transaction {idx}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Get sample of actual fraud cases\n",
    "fraud_sample = X_test[y_test == 1].head(5)\n",
    "non_fraud_sample = X_test[y_test == 0].head(5)\n",
    "\n",
    "# Analyze samples\n",
    "print(\"\\nAnalyzing sample fraud transactions...\")\n",
    "fraud_analysis = analyze_transactions(fraud_sample, y_test.loc[fraud_sample.index], predictor)\n",
    "print(\"\\nAnalyzing sample non-fraud transactions...\")\n",
    "non_fraud_analysis = analyze_transactions(non_fraud_sample, y_test.loc[non_fraud_sample.index], predictor)\n",
    "\n",
    "# Print analysis results\n",
    "print(\"\\nSample Fraud Transactions Analysis:\")\n",
    "for result in fraud_analysis:\n",
    "    print(f\"Transaction {result['index']}: Predicted {'FRAUD' if result['prediction'] else 'OK'} \" \n",
    "          f\"(probability: {result['probability']:.4f}), True label: {'FRAUD' if result['true_label'] else 'OK'}, \"\n",
    "          f\"Correct: {result['correct']}\")\n",
    "    print(f\"  Amount: {result['features']['amount']}, VPN: {result['features']['is_vpn']}\")\n",
    "\n",
    "print(\"\\nSample Non-Fraud Transactions Analysis:\")\n",
    "for result in non_fraud_analysis:\n",
    "    print(f\"Transaction {result['index']}: Predicted {'FRAUD' if result['prediction'] else 'OK'} \" \n",
    "          f\"(probability: {result['probability']:.4f}), True label: {'FRAUD' if result['true_label'] else 'OK'}, \"\n",
    "          f\"Correct: {result['correct']}\")\n",
    "    print(f\"  Amount: {result['features']['amount']}, VPN: {result['features']['is_vpn']}\")\n",
    "\n",
    "\n",
    "\n",
    "#  Create a predictor\n",
    "# predictor = sagemaker.Predictor(\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     sagemaker_session=session,\n",
    "#     serializer=CSVSerializer()\n",
    "# )\n",
    "\n",
    "# # Test on the test set\n",
    "# def format_features_for_prediction(row):\n",
    "#     \"\"\"Format a row of features for prediction\"\"\"\n",
    "#     features_list = []\n",
    "#     for feature in features:\n",
    "#         features_list.append(str(row[feature]))\n",
    "#     return ','.join(features_list)\n",
    "\n",
    "# # Get predictions for test data\n",
    "# print(\"Getting predictions for test data...\")\n",
    "# y_pred_proba = []\n",
    "# batch_size = 100  # Process in batches to avoid throttling\n",
    "\n",
    "# for i in range(0, len(X_test), batch_size):\n",
    "#     batch = X_test.iloc[i:i+batch_size]\n",
    "#     batch_features = [format_features_for_prediction(row) for _, row in batch.iterrows()]\n",
    "    \n",
    "#     # Send each row separately to avoid CSV parsing issues\n",
    "#     batch_predictions = []\n",
    "#     for features_str in batch_features:\n",
    "#         response = predictor.predict(features_str)\n",
    "#         pred = float(response.decode('utf-8'))\n",
    "#         batch_predictions.append(pred)\n",
    "    \n",
    "#     y_pred_proba.extend(batch_predictions)\n",
    "\n",
    "# # Convert probabilities to binary predictions\n",
    "# y_pred = [1 if p >= 0.5 else 0 for p in y_pred_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Model Evaluation:\")\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Create and display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Random prediction line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics for business context\n",
    "fraud_detected = cm[1, 1]  # True positives\n",
    "fraud_missed = cm[1, 0]    # False negatives\n",
    "false_alarms = cm[0, 1]    # False positives\n",
    "correct_negatives = cm[0, 0]  # True negatives\n",
    "\n",
    "# Calculate important business metrics\n",
    "detection_rate = fraud_detected / (fraud_detected + fraud_missed) if (fraud_detected + fraud_missed) > 0 else 0\n",
    "false_positive_rate = false_alarms / (false_alarms + correct_negatives) if (false_alarms + correct_negatives) > 0 else 0\n",
    "precision = fraud_detected / (fraud_detected + false_alarms) if (fraud_detected + false_alarms) > 0 else 0\n",
    "\n",
    "print(\"\\nBusiness Impact Metrics:\")\n",
    "print(f\"Fraud Detection Rate: {detection_rate:.2%}\")\n",
    "print(f\"False Alarm Rate: {false_positive_rate:.2%}\")\n",
    "print(f\"Precision (% of flagged transactions that are actual fraud): {precision:.2%}\")\n",
    "\n",
    "# Save detailed model details for reference\n",
    "model_details = {\n",
    "    'endpoint_name': endpoint_name,\n",
    "    'model_name': model_name,\n",
    "    'features': features,\n",
    "    'evaluation_timestamp': time.time(),\n",
    "    'performance': {\n",
    "        'roc_auc': float(roc_auc),\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
    "        'business_metrics': {\n",
    "            'fraud_detection_rate': float(detection_rate),\n",
    "            'false_alarm_rate': float(false_positive_rate),\n",
    "            'precision': float(precision),\n",
    "            'fraud_cases_detected': int(fraud_detected),\n",
    "            'fraud_cases_missed': int(fraud_missed),\n",
    "            'false_alarms': int(false_alarms)\n",
    "        }\n",
    "    },\n",
    "    'model_parameters': {\n",
    "        'threshold': 0.5,  # Default classification threshold\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to S3\n",
    "model_details_key = f'{prefix}/model-details.json'\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).Object(model_details_key).put(\n",
    "    Body=json.dumps(model_details, indent=2)\n",
    ")\n",
    "print(f\"Model details saved to s3://{bucket}/{model_details_key}\")\n",
    "\n",
    "# Calculate performance at different thresholds to find optimal threshold\n",
    "thresholds_to_try = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "threshold_metrics = []\n",
    "\n",
    "for threshold in thresholds_to_try:\n",
    "    # Convert probabilities to predictions using this threshold\n",
    "    y_pred_at_threshold = [1 if p >= threshold else 0 for p in y_pred_proba]\n",
    "    \n",
    "    # Calculate confusion matrix at this threshold\n",
    "    cm_at_threshold = confusion_matrix(y_test, y_pred_at_threshold)\n",
    "    \n",
    "    # Extract metrics\n",
    "    tp = cm_at_threshold[1, 1]  # True positives\n",
    "    fn = cm_at_threshold[1, 0]  # False negatives\n",
    "    fp = cm_at_threshold[0, 1]  # False positives\n",
    "    tn = cm_at_threshold[0, 0]  # True negatives\n",
    "    \n",
    "    # Calculate rates\n",
    "    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # Store metrics\n",
    "    threshold_metrics.append({\n",
    "        'threshold': threshold,\n",
    "        'detection_rate': detection_rate,\n",
    "        'false_alarm_rate': false_alarm_rate,\n",
    "        'precision': precision,\n",
    "        'true_positives': int(tp),\n",
    "        'false_negatives': int(fn),\n",
    "        'false_positives': int(fp),\n",
    "        'true_negatives': int(tn)\n",
    "    })\n",
    "\n",
    "# Plot metrics across different thresholds\n",
    "plt.figure(figsize=(10, 6))\n",
    "thresholds = [m['threshold'] for m in threshold_metrics]\n",
    "detection_rates = [m['detection_rate'] for m in threshold_metrics]\n",
    "false_alarm_rates = [m['false_alarm_rate'] for m in threshold_metrics]\n",
    "precisions = [m['precision'] for m in threshold_metrics]\n",
    "\n",
    "plt.plot(thresholds, detection_rates, 'bo-', label='Detection Rate')\n",
    "plt.plot(thresholds, false_alarm_rates, 'ro-', label='False Alarm Rate')\n",
    "plt.plot(thresholds, precisions, 'go-', label='Precision')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Performance Metrics at Different Thresholds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRecommended threshold based on business needs:\")\n",
    "# Find threshold with detection rate > 0.8 and lowest false alarm rate\n",
    "good_thresholds = [m for m in threshold_metrics if m['detection_rate'] >= 0.8]\n",
    "if good_thresholds:\n",
    "    recommended = min(good_thresholds, key=lambda x: x['false_alarm_rate'])\n",
    "    print(f\"Threshold: {recommended['threshold']}\")\n",
    "    print(f\"Detection Rate: {recommended['detection_rate']:.2%}\")\n",
    "    print(f\"False Alarm Rate: {recommended['false_alarm_rate']:.2%}\")\n",
    "    print(f\"Precision: {recommended['precision']:.2%}\")\n",
    "else:\n",
    "    print(\"No threshold meets minimum detection rate requirement of 80%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "# print(\"Model Evaluation:\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.ylabel('True Label')\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate ROC AUC\n",
    "# roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "# print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# # Save model details for reference\n",
    "# model_details = {\n",
    "#     'endpoint_name': endpoint_name,\n",
    "#     'model_name': model_name,\n",
    "#     'features': features,\n",
    "#     'performance': {\n",
    "#         'roc_auc': roc_auc,\n",
    "#         'confusion_matrix': cm.tolist()\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Save to S3\n",
    "# model_details_key = f'{prefix}/model-details.json'\n",
    "# s3.Bucket(bucket).Object(model_details_key).put(\n",
    "#     Body=json.dumps(model_details, indent=2)\n",
    "# )\n",
    "# print(f\"Model details saved to s3://{bucket}/{model_details_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Transactions Generator for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "def generate_test_transactions(num_transactions=10, fraud_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Generate test transactions for demonstrating the fraud detection model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    num_transactions : int\n",
    "        Number of transactions to generate\n",
    "    fraud_ratio : float\n",
    "        Ratio of fraudulent transactions to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list: List of transaction dictionaries\n",
    "    \"\"\"\n",
    "    test_transactions = []\n",
    "    \n",
    "    # Calculate counts\n",
    "    num_fraud = int(num_transactions * fraud_ratio)\n",
    "    num_legitimate = num_transactions - num_fraud\n",
    "    \n",
    "    # Generate legitimate transactions\n",
    "    for _ in range(num_legitimate):\n",
    "        transaction = generate_transaction(is_fraud=False)\n",
    "        test_transactions.append(transaction)\n",
    "    \n",
    "    # Generate fraudulent transactions\n",
    "    for _ in range(num_fraud):\n",
    "        transaction = generate_transaction(is_fraud=True)\n",
    "        test_transactions.append(transaction)\n",
    "    \n",
    "    # Shuffle to mix fraud and legitimate\n",
    "    random.shuffle(test_transactions)\n",
    "    \n",
    "    return test_transactions\n",
    "\n",
    "def generate_transaction(is_fraud=False):\n",
    "    \"\"\"Generate a single transaction with realistic properties\"\"\"\n",
    "    # Generate a transaction ID\n",
    "    transaction_id = f\"T{uuid.uuid4().hex[:8].upper()}\"\n",
    "    \n",
    "    # Generate a user ID\n",
    "    user_id = f\"U{random.randint(10000, 99999)}\"\n",
    "    \n",
    "    # Generate timestamp (current time with small random offset)\n",
    "    timestamp = datetime.datetime.now() - datetime.timedelta(\n",
    "        minutes=random.randint(0, 59),\n",
    "        seconds=random.randint(0, 59)\n",
    "    )\n",
    "    timestamp_str = timestamp.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    # Generate amount based on fraud flag\n",
    "    if is_fraud:\n",
    "        amount = round(random.uniform(800, 3000), 2) if random.random() < 0.7 else round(random.uniform(0.5, 20), 2)\n",
    "    else:\n",
    "        amount = round(random.uniform(50, 500), 2)\n",
    "    \n",
    "    # Generate device type\n",
    "    device_options = ['mobile', 'desktop', 'tablet']\n",
    "    if is_fraud:\n",
    "        device_type = random.choices(device_options, weights=[0.7, 0.2, 0.1])[0]\n",
    "    else:\n",
    "        device_type = random.choices(device_options, weights=[0.4, 0.5, 0.1])[0]\n",
    "    \n",
    "    # Generate location\n",
    "    locations = [\n",
    "        'California, USA', 'New York, USA', 'Texas, USA', 'Florida, USA', \n",
    "        'Illinois, USA', 'London, UK', 'Paris, France', 'Berlin, Germany', \n",
    "        'Tokyo, Japan', 'Sydney, Australia'\n",
    "    ]\n",
    "    \n",
    "    if is_fraud:\n",
    "        location = random.choice(locations[5:])  # Foreign locations\n",
    "    else:\n",
    "        location = random.choice(locations[:5])  # US locations\n",
    "    \n",
    "    # Generate VPN usage\n",
    "    if is_fraud:\n",
    "        is_vpn = random.choices([True, False], weights=[0.7, 0.3])[0]\n",
    "    else:\n",
    "        is_vpn = random.choices([True, False], weights=[0.1, 0.9])[0]\n",
    "    \n",
    "    # Generate card type\n",
    "    card_options = ['credit', 'debit', 'gift']\n",
    "    if is_fraud:\n",
    "        card_type = random.choices(card_options, weights=[0.5, 0.2, 0.3])[0]\n",
    "    else:\n",
    "        card_type = random.choices(card_options, weights=[0.4, 0.55, 0.05])[0]\n",
    "    \n",
    "    # Generate status\n",
    "    status_options = ['approved', 'pending', 'declined']\n",
    "    if is_fraud and random.random() < 0.3:\n",
    "        status = random.choices(status_options, weights=[0.6, 0.2, 0.2])[0]\n",
    "    else:\n",
    "        status = random.choices(status_options, weights=[0.95, 0.03, 0.02])[0]\n",
    "    \n",
    "    # Create transaction\n",
    "    return {\n",
    "        'transaction_id': transaction_id,\n",
    "        'user_id': user_id,\n",
    "        'timestamp': timestamp_str,\n",
    "        'amount': amount,\n",
    "        'device_type': device_type,\n",
    "        'location': location,\n",
    "        'is_vpn': is_vpn,\n",
    "        'card_type': card_type,\n",
    "        'status': status\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    transactions = generate_test_transactions(20, fraud_ratio=0.3)\n",
    "    print(f\"Generated {len(transactions)} transactions\")\n",
    "    print(f\"Sample transaction: {transactions[0]}\")\n",
    "\n",
    "# def generate_test_transactions(num_transactions=10, fraud_ratio=0.3):\n",
    "#     \"\"\"Generate test transactions for demonstrating the model\"\"\"\n",
    "#     from package.generator import generate_transaction\n",
    "    \n",
    "#     test_transactions = []\n",
    "    \n",
    "#     # Generate mostly legitimate transactions\n",
    "#     for _ in range(int(num_transactions * (1 - fraud_ratio))):\n",
    "#         transaction = generate_transaction(fraud_probability=0)\n",
    "#         test_transactions.append(transaction)\n",
    "    \n",
    "#     # Generate some fraudulent transactions\n",
    "#     for _ in range(int(num_transactions * fraud_ratio)):\n",
    "#         transaction = generate_transaction(fraud_probability=1)\n",
    "#         test_transactions.append(transaction)\n",
    "    \n",
    "#     return test_transactions\n",
    "\n",
    "# # Generate test transactions\n",
    "# test_transactions = generate_test_transactions(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class TransactionFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Feature extractor for fraud detection model inference.\n",
    "    Prepares transaction data for the ML model in production.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, s3_bucket=None, feature_info_key=None, dynamodb_table=None):\n",
    "        \"\"\"\n",
    "        Initialize the feature extractor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        s3_bucket : str\n",
    "            S3 bucket containing feature info and model metadata\n",
    "        feature_info_key : str\n",
    "            S3 key for the feature info JSON file\n",
    "        dynamodb_table : str\n",
    "            DynamoDB table containing historical user transaction data\n",
    "        \"\"\"\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.dynamodb = boto3.resource('dynamodb')\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.feature_info_key = feature_info_key\n",
    "        self.dynamodb_table = dynamodb_table\n",
    "        \n",
    "        # Default feature lists in case we can't load from S3\n",
    "        self.feature_names = []\n",
    "        self.categorical_features = []\n",
    "        self.numeric_features = []\n",
    "        \n",
    "        # Load feature info if provided\n",
    "        if s3_bucket and feature_info_key:\n",
    "            self.load_feature_info()\n",
    "        \n",
    "        # Cache for location and device fraud rates\n",
    "        self.location_fraud_rates = {}\n",
    "        self.device_fraud_rates = {}\n",
    "        self.card_fraud_rates = {}\n",
    "        \n",
    "        # Load fraud rates if possible\n",
    "        self.load_fraud_rates()\n",
    "    \n",
    "    def load_feature_info(self):\n",
    "        \"\"\"Load feature information from S3\"\"\"\n",
    "        try:\n",
    "            response = self.s3_client.get_object(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Key=self.feature_info_key\n",
    "            )\n",
    "            feature_info = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            \n",
    "            self.feature_names = feature_info.get('feature_names', [])\n",
    "            self.categorical_features = feature_info.get('categorical_features', [])\n",
    "            self.numeric_features = feature_info.get('numeric_features', [])\n",
    "            \n",
    "            logger.info(f\"Loaded feature info with {len(self.feature_names)} features\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading feature info: {str(e)}\")\n",
    "    \n",
    "    def load_fraud_rates(self):\n",
    "        \"\"\"Load fraud rates for categorical features if available\"\"\"\n",
    "        try:\n",
    "            fraud_rates_key = 'models/fraud_rates.json'\n",
    "            response = self.s3_client.get_object(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Key=fraud_rates_key\n",
    "            )\n",
    "            fraud_rates = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            \n",
    "            self.location_fraud_rates = fraud_rates.get('location', {})\n",
    "            self.device_fraud_rates = fraud_rates.get('device_type', {})\n",
    "            self.card_fraud_rates = fraud_rates.get('card_type', {})\n",
    "            \n",
    "            logger.info(\"Loaded fraud rates for categorical features\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load fraud rates, using defaults: {str(e)}\")\n",
    "            # Set default rates based on historical data analysis\n",
    "            self.location_fraud_rates = {\n",
    "                'California, USA': 0.02, 'New York, USA': 0.02, 'Texas, USA': 0.02,\n",
    "                'Florida, USA': 0.03, 'Illinois, USA': 0.02, 'London, UK': 0.03,\n",
    "                'Paris, France': 0.04, 'Berlin, Germany': 0.04, 'Tokyo, Japan': 0.05,\n",
    "                'Sydney, Australia': 0.05, 'Unknown': 0.09\n",
    "            }\n",
    "            self.device_fraud_rates = {'mobile': 0.03, 'desktop': 0.01, 'tablet': 0.02}\n",
    "            self.card_fraud_rates = {'credit': 0.025, 'debit': 0.01, 'gift': 0.05}\n",
    "    \n",
    "    def get_user_stats(self, user_id):\n",
    "        \"\"\"\n",
    "        Get user transaction statistics from DynamoDB.\n",
    "        In production, this would query a database of user history.\n",
    "        \n",
    "        Returns a dictionary of user stats or None if not found\n",
    "        \"\"\"\n",
    "        if not self.dynamodb_table:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            table = self.dynamodb.Table(self.dynamodb_table)\n",
    "            response = table.get_item(Key={'user_id': user_id})\n",
    "            \n",
    "            if 'Item' in response:\n",
    "                return response['Item'].get('stats', {})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving user stats: {str(e)}\")\n",
    "        \n",
    "        # Return default values if no data found or error\n",
    "        return {\n",
    "            'transaction_count': 1,\n",
    "            'avg_amount': 0,\n",
    "            'std_amount': 0,\n",
    "            'max_amount': 0,\n",
    "            'vpn_ratio': 0\n",
    "        }\n",
    "    \n",
    "    def extract_features(self, transaction):\n",
    "        \"\"\"\n",
    "        Extract features from a transaction for model prediction\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        transaction : dict\n",
    "            Transaction data dictionary\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str: CSV-formatted feature string for model prediction\n",
    "        \"\"\"\n",
    "        # Create a dictionary to hold all features\n",
    "        features_dict = {}\n",
    "        \n",
    "        # Basic transaction features\n",
    "        features_dict['amount'] = float(transaction.get('amount', 0))\n",
    "        features_dict['is_vpn'] = 1 if transaction.get('is_vpn', False) else 0\n",
    "        \n",
    "        # Time-based features\n",
    "        try:\n",
    "            if isinstance(transaction.get('timestamp'), str):\n",
    "                timestamp = datetime.datetime.strptime(\n",
    "                    transaction.get('timestamp'),\n",
    "                    '%Y-%m-%dT%H:%M:%SZ'\n",
    "                )\n",
    "            else:\n",
    "                timestamp = transaction.get('timestamp', datetime.datetime.now())\n",
    "                \n",
    "            features_dict['hour_of_day'] = timestamp.hour\n",
    "            features_dict['day_of_week'] = timestamp.weekday()\n",
    "            features_dict['month'] = timestamp.month\n",
    "            features_dict['day_of_month'] = timestamp.day\n",
    "            features_dict['is_weekend'] = 1 if timestamp.weekday() >= 5 else 0\n",
    "            features_dict['is_night'] = 1 if (timestamp.hour < 6 or timestamp.hour >= 22) else 0\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing timestamp: {str(e)}\")\n",
    "            features_dict['hour_of_day'] = 0\n",
    "            features_dict['day_of_week'] = 0\n",
    "            features_dict['month'] = 1\n",
    "            features_dict['day_of_month'] = 1\n",
    "            features_dict['is_weekend'] = 0\n",
    "            features_dict['is_night'] = 0\n",
    "        \n",
    "        # Get user statistics for behavioral features\n",
    "        user_id = transaction.get('user_id')\n",
    "        user_stats = self.get_user_stats(user_id)\n",
    "        \n",
    "        if user_stats:\n",
    "            # User transaction count\n",
    "            features_dict['user_transaction_count'] = user_stats.get('transaction_count', 1)\n",
    "            \n",
    "            # User average amount\n",
    "            user_avg_amount = user_stats.get('avg_amount', 0)\n",
    "            features_dict['user_avg_amount'] = user_avg_amount\n",
    "            \n",
    "            # User amount standard deviation\n",
    "            user_amount_std = user_stats.get('std_amount', 1)\n",
    "            features_dict['user_amount_std'] = user_amount_std\n",
    "            \n",
    "            # User min and max amount\n",
    "            features_dict['user_min_amount'] = user_stats.get('min_amount', 0)\n",
    "            features_dict['user_max_amount'] = user_stats.get('max_amount', 0)\n",
    "            \n",
    "            # VPN usage ratio\n",
    "            features_dict['user_vpn_ratio'] = user_stats.get('vpn_ratio', 0)\n",
    "            \n",
    "            # Calculate z-score\n",
    "            if user_amount_std > 0:\n",
    "                features_dict['amount_zscore'] = (features_dict['amount'] - user_avg_amount) / user_amount_std\n",
    "            else:\n",
    "                features_dict['amount_zscore'] = 0\n",
    "                \n",
    "            # Calculate amount to max ratio\n",
    "            max_amount = user_stats.get('max_amount', 0)\n",
    "            if max_amount > 0:\n",
    "                features_dict['amount_to_max_ratio'] = features_dict['amount'] / max_amount\n",
    "            else:\n",
    "                features_dict['amount_to_max_ratio'] = 1\n",
    "                \n",
    "            # Is this an unusual amount for the user?\n",
    "            is_unusual = 0\n",
    "            if user_amount_std > 0 and abs(features_dict['amount_zscore']) > 2:\n",
    "                is_unusual = 1\n",
    "            features_dict['is_unusual_amount'] = is_unusual\n",
    "            \n",
    "            # Is this a new user?\n",
    "            features_dict['is_new_user'] = 1 if features_dict['user_transaction_count'] <= 3 else 0\n",
    "            \n",
    "            # Is this the user's largest transaction?\n",
    "            features_dict['is_largest_tx'] = 1 if features_dict['amount'] >= max_amount * 0.95 else 0\n",
    "        else:\n",
    "            # Default values for new users or if user data not available\n",
    "            features_dict['user_transaction_count'] = 1\n",
    "            features_dict['user_avg_amount'] = 0\n",
    "            features_dict['user_amount_std'] = 1\n",
    "            features_dict['user_min_amount'] = 0\n",
    "            features_dict['user_max_amount'] = 0\n",
    "            features_dict['user_vpn_ratio'] = 0\n",
    "            features_dict['amount_zscore'] = 0\n",
    "            features_dict['amount_to_max_ratio'] = 1\n",
    "            features_dict['is_unusual_amount'] = 0\n",
    "            features_dict['is_new_user'] = 1\n",
    "            features_dict['is_largest_tx'] = 1\n",
    "        \n",
    "        # Location risk score based on historical fraud rates\n",
    "        location = transaction.get('location', 'Unknown')\n",
    "        features_dict['location_fraud_rate'] = self.location_fraud_rates.get(location, 0.05)\n",
    "        \n",
    "        # Device fraud rate\n",
    "        device_type = transaction.get('device_type', 'unknown')\n",
    "        features_dict['device_fraud_rate'] = self.device_fraud_rates.get(device_type, 0.02)\n",
    "        \n",
    "        # Card fraud rate\n",
    "        card_type = transaction.get('card_type', 'unknown')\n",
    "        features_dict['card_fraud_rate'] = self.card_fraud_rates.get(card_type, 0.02)\n",
    "        \n",
    "        # Create one-hot encoded features for categorical variables\n",
    "        # Location one-hot encoding\n",
    "        for loc in self.location_fraud_rates.keys():\n",
    "            features_dict[f'location_{loc.replace(\", \", \"_\").lower()}'] = 1 if location == loc else 0\n",
    "            \n",
    "        # Device type one-hot encoding\n",
    "        for dev in ['mobile', 'desktop', 'tablet']:\n",
    "            features_dict[f'device_type_{dev}'] = 1 if device_type == dev else 0\n",
    "            \n",
    "        # Card type one-hot encoding\n",
    "        for card in ['credit', 'debit', 'gift']:\n",
    "            features_dict[f'card_type_{card}'] = 1 if card_type == card else 0\n",
    "            \n",
    "        # Transaction status one-hot encoding\n",
    "        status = transaction.get('status', 'approved')\n",
    "        for st in ['approved', 'pending', 'declined']:\n",
    "            features_dict[f'status_{st}'] = 1 if status == st else 0\n",
    "        \n",
    "        # Assemble feature string in the correct order based on feature names\n",
    "        feature_values = []\n",
    "        for feature in self.feature_names:\n",
    "            if feature in features_dict:\n",
    "                feature_values.append(str(features_dict[feature]))\n",
    "            else:\n",
    "                feature_values.append('0')  # Default value for missing features\n",
    "        \n",
    "        return ','.join(feature_values)\n",
    "    \n",
    "    def format_batch_for_prediction(self, transactions):\n",
    "        \"\"\"\n",
    "        Format a batch of transactions for prediction\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        transactions : list\n",
    "            List of transaction dictionaries\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list: List of CSV-formatted feature strings\n",
    "        \"\"\"\n",
    "        return [self.extract_features(tx) for tx in transactions]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def process_transactions_for_prediction(transactions, model_bucket=None):\n",
    "    \"\"\"\n",
    "    Process transactions and format them for model prediction\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    transactions : list\n",
    "        List of transaction dictionaries\n",
    "    model_bucket : str, optional\n",
    "        S3 bucket containing model metadata\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list: List of CSV-formatted feature strings for model prediction\n",
    "    \"\"\"\n",
    "    # Initialize feature extractor\n",
    "    extractor = TransactionFeatureExtractor(\n",
    "        s3_bucket=model_bucket or os.environ.get('MODEL_DATA_S3_BUCKET', 'ssense-fraud-model-data'),\n",
    "        feature_info_key='fraud-classifier/model/feature_info.json',\n",
    "        dynamodb_table=os.environ.get('TRANSACTION_HISTORY_TABLE', 'ssense-fraud-user-history')\n",
    "    )\n",
    "    \n",
    "    # Process transactions\n",
    "    return extractor.format_batch_for_prediction(transactions)\n",
    "\n",
    "\n",
    "# # Format transactions for prediction\n",
    "# def format_transaction_for_prediction(transaction):\n",
    "#     \"\"\"Format a transaction for model prediction\"\"\"\n",
    "#     # Extract features from the transaction\n",
    "#     features_dict = {}\n",
    "    \n",
    "#     # Basic features\n",
    "#     features_dict['amount'] = float(transaction.get('amount', 0))\n",
    "#     features_dict['is_vpn'] = 1 if transaction.get('is_vpn', False) else 0\n",
    "    \n",
    "#     # Time-based features\n",
    "#     import datetime\n",
    "#     try:\n",
    "#         timestamp = datetime.datetime.strptime(\n",
    "#             transaction.get('timestamp', datetime.datetime.now().isoformat()),\n",
    "#             '%Y-%m-%dT%H:%M:%SZ'\n",
    "#         )\n",
    "#         features_dict['hour_of_day'] = timestamp.hour\n",
    "#         features_dict['day_of_week'] = timestamp.weekday()\n",
    "#         features_dict['is_weekend'] = 1 if timestamp.weekday() >= 5 else 0\n",
    "#         features_dict['is_night'] = 1 if (timestamp.hour < 6 or timestamp.hour >= 22) else 0\n",
    "#     except:\n",
    "#         features_dict['hour_of_day'] = 0\n",
    "#         features_dict['day_of_week'] = 0\n",
    "#         features_dict['is_weekend'] = 0\n",
    "#         features_dict['is_night'] = 0\n",
    "    \n",
    "#     # Location risk\n",
    "#     location_risk = {\n",
    "#         'California, USA': 0.2,\n",
    "#         'New York, USA': 0.2,\n",
    "#         'Texas, USA': 0.2,\n",
    "#         'Florida, USA': 0.3,\n",
    "#         'Illinois, USA': 0.2,\n",
    "#         'London, UK': 0.3,\n",
    "#         'Paris, France': 0.4,\n",
    "#         'Berlin, Germany': 0.4,\n",
    "#         'Tokyo, Japan': 0.5,\n",
    "#         'Sydney, Australia': 0.5,\n",
    "#         'Unknown': 0.9\n",
    "#     }\n",
    "#     features_dict['location_risk'] = location_risk.get(transaction.get('location', 'Unknown'), 0.7)\n",
    "    \n",
    "#     # Mock user behavior features (would come from historical data in production)\n",
    "#     features_dict['user_transaction_count'] = 5  # Dummy value\n",
    "#     features_dict['amount_zscore'] = (features_dict['amount'] - 200) / 100  # Dummy calculation\n",
    "    \n",
    "#     # Transaction risk score\n",
    "#     features_dict['transaction_risk_score'] = (\n",
    "#         features_dict['amount_zscore'] * 0.3 +\n",
    "#         features_dict['location_risk'] * 0.2 +\n",
    "#         features_dict['is_vpn'] * 0.2 +\n",
    "#         features_dict['is_night'] * 0.1\n",
    "#     )\n",
    "    \n",
    "#     # One-hot encoded categorical features\n",
    "#     device_type = transaction.get('device_type', 'unknown')\n",
    "#     features_dict[f'devicetype{device_type}'] = 1\n",
    "    \n",
    "#     card_type = transaction.get('card_type', 'unknown')\n",
    "#     features_dict[f'card_type_{card_type}'] = 1\n",
    "    \n",
    "#     status = transaction.get('status', 'approved')\n",
    "#     features_dict[f'status_{status}'] = 1\n",
    "    \n",
    "#     # Assemble feature string in the correct order\n",
    "#     feature_values = []\n",
    "#     for feature in features:\n",
    "#         feature_values.append(str(features_dict.get(feature, 0)))\n",
    "    \n",
    "#     return ','.join(feature_values)\n",
    "\n",
    "# # Test the transactions against the model\n",
    "# print(\"Testing transactions against the model...\")\n",
    "# for transaction in test_transactions:\n",
    "#     features_str = format_transaction_for_prediction(transaction)\n",
    "    \n",
    "#     # Get prediction\n",
    "#     response = predictor.predict(features_str)\n",
    "#     fraud_probability = float(response.decode('utf-8'))\n",
    "#     is_fraud = fraud_probability >= 0.5\n",
    "    \n",
    "#     print(f\"Transaction {transaction['transaction_id']}:\")\n",
    "#     print(f\"Amount: ${transaction['amount']:.2f}, Location: {transaction['location']}\")\n",
    "#     print(f\"Device: {transaction['device_type']}, VPN: {transaction['is_vpn']}\")\n",
    "#     print(f\"Prediction: {'FRAUD' if is_fraud else 'LEGITIMATE'} (probability: {fraud_probability:.4f})\")\n",
    "#     print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinesis Integration for Real-time Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_transactions_to_kinesis(transactions, stream_name=config.KINESIS_STREAM_NAME):\n",
    "    \"\"\"Send transactions to Kinesis for real-time processing\"\"\"\n",
    "    kinesis = boto3.client('kinesis')\n",
    "    \n",
    "    for transaction in transactions:\n",
    "        # Convert transaction to JSON\n",
    "        transaction_json = json.dumps(transaction)\n",
    "        \n",
    "        # Send to Kinesis\n",
    "        response = kinesis.put_record(\n",
    "            StreamName=stream_name,\n",
    "            Data=transaction_json,\n",
    "            PartitionKey=transaction['transaction_id']\n",
    "        )\n",
    "        \n",
    "        print(f\"Sent transaction {transaction['transaction_id']} to Kinesis: Shard {response['ShardId']}\")\n",
    "        \n",
    "        # Small delay to avoid throttling\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Generate a batch of transactions and send to Kinesis\n",
    "kinesis_test_transactions = generate_test_transactions(10, fraud_ratio=0.2)\n",
    "send_transactions_to_kinesis(kinesis_test_transactions)\n",
    "\n",
    "print(f\"Sent {len(kinesis_test_transactions)} transactions to Kinesis stream {config.KINESIS_STREAM_NAME}\")\n",
    "print(\"These will be processed by the Lambda function and results stored in DynamoDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard for Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dynamodb_for_results(transaction_ids, table_name=config.DYNAMODB_TABLE):\n",
    "    \"\"\"Check DynamoDB for processing results of test transactions\"\"\"\n",
    "    dynamodb = boto3.resource('dynamodb')\n",
    "    table = dynamodb.Table(table_name)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for tx_id in transaction_ids:\n",
    "        try:\n",
    "            response = table.get_item(Key={'transaction_id': tx_id})\n",
    "            if 'Item' in response:\n",
    "                results.append(response['Item'])\n",
    "            else:\n",
    "                print(f\"Transaction {tx_id} not found in DynamoDB yet.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving transaction {tx_id}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Give Lambda some time to process\n",
    "print(\"Waiting for Lambda to process transactions...\")\n",
    "time.sleep(10)  # Wait 10 seconds\n",
    "\n",
    "# Check DynamoDB for results\n",
    "tx_ids = [tx['transaction_id'] for tx in kinesis_test_transactions]\n",
    "results = check_dynamodb_for_results(tx_ids)\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"Found {len(results)} transactions in DynamoDB\")\n",
    "    \n",
    "    # Plot fraud probabilities\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='transaction_id', y='fraud_probability', data=results_df)\n",
    "    plt.title('Fraud Probabilities for Test Transactions')\n",
    "    plt.xlabel('Transaction ID')\n",
    "    plt.ylabel('Fraud Probability')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results found in DynamoDB yet. Lambda might still be processing or there might be an issue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explanation and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_fraud_prediction(transaction, fraud_probability):\n",
    "    \"\"\"Explain why a transaction is flagged as fraudulent\"\"\"\n",
    "    risk_factors = []\n",
    "    \n",
    "    # Check for high amount\n",
    "    if transaction['amount'] > 500:\n",
    "        risk_factors.append(f\"High transaction amount: ${transaction['amount']:.2f}\")\n",
    "    \n",
    "    # Check for VPN usage\n",
    "    if transaction.get('is_vpn', False):\n",
    "        risk_factors.append(\"VPN usage detected\")\n",
    "    \n",
    "    # Check for unusual location\n",
    "    location = transaction.get('location', '')\n",
    "    if location in ['Tokyo, Japan', 'Berlin, Germany', 'Paris, France', 'Sydney, Australia']:\n",
    "        risk_factors.append(f\"Transaction from unusual location: {location}\")\n",
    "    \n",
    "    # Check for gift card\n",
    "    if transaction.get('card_type', '') == 'gift':\n",
    "        risk_factors.append(\"Gift card usage\")\n",
    "    \n",
    "    # Check for mobile device\n",
    "    if transaction.get('device_type', '') == 'mobile':\n",
    "        risk_factors.append(\"Mobile device used\")\n",
    "    \n",
    "    # Check for declined status\n",
    "    if transaction.get('status', '') == 'declined':\n",
    "        risk_factors.append(\"Transaction was initially declined\")\n",
    "    \n",
    "    # Check for night-time transaction\n",
    "    try:\n",
    "        timestamp = datetime.datetime.strptime(\n",
    "            transaction.get('timestamp', ''),\n",
    "            '%Y-%m-%dT%H:%M:%SZ'\n",
    "        )\n",
    "        hour = timestamp.hour\n",
    "        if hour < 6 or hour >= 22:\n",
    "            risk_factors.append(f\"Transaction occurred during night hours: {hour}:00\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'transaction_id': transaction['transaction_id'],\n",
    "        'fraud_probability': fraud_probability,\n",
    "        'risk_score': fraud_probability,\n",
    "        'is_fraud': fraud_probability > 0.5,\n",
    "        'risk_factors': risk_factors,\n",
    "        'explanation': f\"Transaction flagged with {fraud_probability:.1%} fraud probability due to {len(risk_factors)} risk factors.\"\n",
    "    }\n",
    "\n",
    "# Demonstrate explanation for a high-risk transaction\n",
    "high_risk_transactions = [tx for tx in test_transactions if tx.get('is_vpn', False) and tx.get('amount', 0) > 500]\n",
    "if high_risk_transactions:\n",
    "    sample_tx = high_risk_transactions[0]\n",
    "    features_str = format_transaction_for_prediction(sample_tx)\n",
    "    \n",
    "    # Get prediction\n",
    "    response = predictor.predict(features_str)\n",
    "    fraud_probability = float(response.decode('utf-8'))\n",
    "    \n",
    "    # Get explanation\n",
    "    explanation = explain_fraud_prediction(sample_tx, fraud_probability)\n",
    "    \n",
    "    print(\"Sample Fraud Explanation:\")\n",
    "    print(f\"Transaction: {sample_tx['transaction_id']}\")\n",
    "    print(f\"Amount: ${sample_tx['amount']:.2f}, Location: {sample_tx['location']}\")\n",
    "    print(f\"Fraud Probability: {fraud_probability:.2%}\")\n",
    "    print(\"Risk Factors:\")\n",
    "    for factor in explanation['risk_factors']:\n",
    "        print(f\"- {factor}\")\n",
    "    print(f\"Explanation: {explanation['explanation']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

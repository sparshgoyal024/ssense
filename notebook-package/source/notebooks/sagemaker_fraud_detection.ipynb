{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-commerce Fraud Detection using SageMaker\n",
    "\n",
    "This notebook demonstrates how to build a machine learning model to detect fraudulent e-commerce transactions. We'll use SageMaker to train and deploy the model, then test it on simulated transaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import io\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('./src/'))\n",
    "from package import config\n",
    "\n",
    "# Initialize AWS clients\n",
    "session = sagemaker.Session()\n",
    "s3 = boto3.resource('s3')\n",
    "sm_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Enhance the existing data generation with more fraud patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_enhanced_transaction_data(num_transactions=10000, fraud_ratio=0.1):\n",
    "    \"\"\"Generate enhanced transaction data with more sophisticated fraud patterns\"\"\"\n",
    "    from package.generator import generate_transaction\n",
    "    \n",
    "    transactions = []\n",
    "    \n",
    "    # Generate legitimate transactions\n",
    "    for _ in range(int(num_transactions * (1 - fraud_ratio))):\n",
    "        transactions.append(generate_transaction(fraud_probability=0))\n",
    "    \n",
    "    # Generate fraudulent transactions with specific patterns\n",
    "    for _ in range(int(num_transactions * fraud_ratio)):\n",
    "        transaction = generate_transaction(fraud_probability=1)\n",
    "        \n",
    "        # Add more sophisticated fraud patterns\n",
    "        if np.random.random() < 0.3:\n",
    "            # Pattern 1: High value transactions from unusual locations using VPN\n",
    "            transaction['amount'] = float(np.random.uniform(1000, 5000))\n",
    "            transaction['location'] = np.random.choice(['Tokyo, Japan', 'Berlin, Germany', 'Sydney, Australia'])\n",
    "            transaction['is_vpn'] = True\n",
    "        elif np.random.random() < 0.5:\n",
    "            # Pattern 2: Multiple small transactions using gift cards\n",
    "            transaction['amount'] = float(np.random.uniform(50, 200))\n",
    "            transaction['card_type'] = 'gift'\n",
    "            transaction['device_type'] = 'mobile'\n",
    "        else:\n",
    "            # Pattern 3: Declined transactions attempted again\n",
    "            transaction['status'] = 'declined'\n",
    "            transaction['is_vpn'] = np.random.choice([True, False], p=[0.7, 0.3])\n",
    "        \n",
    "        transactions.append(transaction)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(transactions)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate transaction data with enhanced fraud patterns\n",
    "num_transactions = 20000  # Increase dataset size\n",
    "fraud_ratio = 0.1        # 10% fraud rate for better training\n",
    "df = generate_enhanced_transaction_data(num_transactions, fraud_ratio)\n",
    "\n",
    "# Display sample data\n",
    "print(f\"Generated {len(df)} transactions\")\n",
    "print(f\"Fraud percentage: {df['is_vpn'].mean() * 100:.2f}%\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Feature Engineering\n",
    "\n",
    "Add more sophisticated feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"More sophisticated feature engineering for fraud detection\"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df_features['timestamp'] = pd.to_datetime(df_features['timestamp'])\n",
    "    df_features['hour_of_day'] = df_features['timestamp'].dt.hour\n",
    "    df_features['day_of_week'] = df_features['timestamp'].dt.dayofweek\n",
    "    df_features['is_weekend'] = df_features['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df_features['is_night'] = df_features['hour_of_day'].apply(lambda x: 1 if (x < 6 or x >= 22) else 0)\n",
    "    \n",
    "    # Location risk score\n",
    "    location_risk = {\n",
    "        'California, USA': 0.2,\n",
    "        'New York, USA': 0.2,\n",
    "        'Texas, USA': 0.2,\n",
    "        'Florida, USA': 0.3,\n",
    "        'Illinois, USA': 0.2,\n",
    "        'London, UK': 0.3,\n",
    "        'Paris, France': 0.4,\n",
    "        'Berlin, Germany': 0.4,\n",
    "        'Tokyo, Japan': 0.5,\n",
    "        'Sydney, Australia': 0.5,\n",
    "        'Unknown': 0.9\n",
    "    }\n",
    "    df_features['location_risk'] = df_features['location'].map(location_risk)\n",
    "    \n",
    "    # User behavior features\n",
    "    # Group by user ID to analyze user patterns\n",
    "    user_transaction_counts = df_features.groupby('user_id').size().reset_index(name='user_transaction_count')\n",
    "    df_features = df_features.merge(user_transaction_counts, on='user_id', how='left')\n",
    "    \n",
    "    user_amount_avg = df_features.groupby('user_id')['amount'].mean().reset_index(name='user_avg_amount')\n",
    "    df_features = df_features.merge(user_amount_avg, on='user_id', how='left')\n",
    "    \n",
    "    # Transaction amount z-score (compared to user's average)\n",
    "    df_features['amount_zscore'] = df_features.apply(\n",
    "        lambda row: (row['amount'] - row['user_avg_amount']) / df_features['amount'].std() \n",
    "        if row['user_transaction_count'] > 1 else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    # Device-location mismatch (unusual device for location)\n",
    "    df_features['device_type_num'] = df_features['device_type'].map({\n",
    "        'mobile': 0, 'desktop': 1, 'tablet': 2\n",
    "    })\n",
    "    \n",
    "    # Create dummies for categorical variables\n",
    "    df_features = pd.get_dummies(\n",
    "        df_features, \n",
    "        columns=['device_type', 'card_type', 'status'], \n",
    "        drop_first=False\n",
    "    )\n",
    "    \n",
    "    # Convert boolean to integer\n",
    "    df_features['is_vpn'] = df_features['is_vpn'].astype(int)\n",
    "    \n",
    "    # Define a fraud label - this depends on how fraud is defined in your synthetic data\n",
    "    # In this case, we'll use is_vpn + high amount + unusual location as indicators\n",
    "    if 'is_fraud' not in df_features.columns:\n",
    "        df_features['is_fraud'] = ((df_features['is_vpn'] == 1) & \n",
    "                                  (df_features['amount'] > 500) & \n",
    "                                  (df_features['location_risk'] > 0.3)).astype(int)\n",
    "    \n",
    "    # Compute additional risk factors\n",
    "    df_features['transaction_risk_score'] = (\n",
    "        df_features['amount_zscore'] * 0.3 +\n",
    "        df_features['location_risk'] * 0.2 +\n",
    "        df_features['is_vpn'] * 0.2 +\n",
    "        df_features['is_night'] * 0.1\n",
    "    )\n",
    "    \n",
    "    # Add status_declined if it exists\n",
    "    if 'status_declined' in df_features.columns:\n",
    "        df_features['transaction_risk_score'] += df_features['status_declined'] * 0.2\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply feature engineering\n",
    "df_engineered = engineer_features(df)\n",
    "\n",
    "# Display feature correlations with fraud\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_engineered.corr()\n",
    "fraud_correlations = correlation_matrix['is_fraud'].sort_values(ascending=False)\n",
    "print(\"Top features correlated with fraud:\")\n",
    "print(fraud_correlations.head(10))\n",
    "\n",
    "# Plot correlation heatmap\n",
    "sns.heatmap(correlation_matrix.iloc[:15, :15], annot=True, cmap='coolwarm')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for model training\n",
    "features = ['amount', 'hour_of_day', 'day_of_week', 'is_weekend', 'is_night',\n",
    "            'location_risk', 'is_vpn', 'user_transaction_count', 'amount_zscore',\n",
    "            'transaction_risk_score']\n",
    "\n",
    "# Add categorical columns (from one-hot encoding)\n",
    "categorical_columns = [col for col in df_engineered.columns if \n",
    "                      col.startswith('device_type_') or \n",
    "                      col.startswith('card_type_') or \n",
    "                      col.startswith('status_')]\n",
    "features.extend(categorical_columns)\n",
    "\n",
    "# Define target\n",
    "target = 'is_fraud'\n",
    "\n",
    "# Split the data\n",
    "X = df_engineered[features]\n",
    "y = df_engineered[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(f\"Fraud ratio in training: {y_train.mean():.2f}\")\n",
    "print(f\"Fraud ratio in testing: {y_test.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using SageMaker built-in XGBoost algorithm\n",
    "# First, prepare data in SVM light format required by SageMaker XGBoost\n",
    "train_file = io.BytesIO()\n",
    "dump_svmlight_file(X_train, y_train, train_file)\n",
    "train_file.seek(0)\n",
    "\n",
    "# Upload data to S3\n",
    "bucket = config.MODEL_DATA_S3_BUCKET\n",
    "prefix = 'fraud-classifier'\n",
    "key = f'{prefix}/train/train.libsvm'\n",
    "\n",
    "s3.Bucket(bucket).Object(key).upload_fileobj(train_file)\n",
    "train_data_s3_uri = f's3://{bucket}/{key}'\n",
    "print(f\"Uploaded training data to {train_data_s3_uri}\")\n",
    "\n",
    "# Set output location\n",
    "output_s3_uri = f's3://{bucket}/{prefix}/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data in SVM light format required by SageMaker XGBoost\n",
    "# First properly split into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create train file\n",
    "train_file = io.BytesIO()\n",
    "dump_svmlight_file(X_train, y_train, train_file)\n",
    "train_file.seek(0)\n",
    "\n",
    "# Create validation file\n",
    "validation_file = io.BytesIO()\n",
    "dump_svmlight_file(X_val, y_val, validation_file)\n",
    "validation_file.seek(0)\n",
    "\n",
    "# Upload data to S3\n",
    "bucket = MODEL_DATA_S3_BUCKET\n",
    "prefix = 'fraud-classifier'\n",
    "\n",
    "# Upload train data\n",
    "train_key = f'{prefix}/train/train.libsvm'\n",
    "s3.Bucket(bucket).Object(train_key).upload_fileobj(train_file)\n",
    "train_data_s3_uri = f's3://{bucket}/{train_key}'\n",
    "\n",
    "# Upload validation data\n",
    "val_key = f'{prefix}/validation/validation.libsvm'\n",
    "s3.Bucket(bucket).Object(val_key).upload_fileobj(validation_file)\n",
    "validation_data_s3_uri = f's3://{bucket}/{val_key}'\n",
    "\n",
    "# Set output location\n",
    "output_s3_uri = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "print(f\"Uploaded training data to {train_data_s3_uri}\")\n",
    "print(f\"Uploaded validation data to {validation_data_s3_uri}\")\n",
    "\n",
    "# Get the XGBoost image - use the newer SageMaker API\n",
    "from sagemaker import image_uris\n",
    "container = image_uris.retrieve(\"xgboost\", boto3.Session().region_name, version=\"1.0-1\")\n",
    "\n",
    "# Set up hyperparameter tuning job\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.01, 0.3),\n",
    "    'gamma': ContinuousParameter(0, 5),\n",
    "    'min_child_weight': IntegerParameter(1, 10),\n",
    "    'subsample': ContinuousParameter(0.5, 1.0),\n",
    "    'colsample_bytree': ContinuousParameter(0.5, 1.0)\n",
    "}\n",
    "\n",
    "# Create an estimator with both train and validation channels\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=SAGEMAKER_IAM_ROLE,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    output_path=output_s3_uri,\n",
    "    sagemaker_session=session,\n",
    "    base_job_name='fraud-detection-xgb'\n",
    ")\n",
    "\n",
    "# Set static hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    num_round=100,\n",
    "    rate_drop=0.1,\n",
    "    scale_pos_weight=10  # Helpful for imbalanced datasets\n",
    ")\n",
    "\n",
    "# Create the tuner with the correct metric name\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb,\n",
    "    'validation:auc',  # Make sure this matches eval_metric\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=5,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type='Maximize'\n",
    ")\n",
    "\n",
    "# Start the hyperparameter tuning job with both train and validation\n",
    "tuner.fit({\n",
    "    'train': train_data_s3_uri,\n",
    "    'validation': validation_data_s3_uri\n",
    "})\n",
    "print(\"Hyperparameter tuning job started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from hyperparameter tuning\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "best_job_name = sm_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")['BestTrainingJob']['TrainingJobName']\n",
    "\n",
    "# Create model\n",
    "model_name = f\"fraud-detection-model-{int(time.time())}\"\n",
    "model_info = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': f\"{output_s3_uri}/{best_job_name}/output/model.tar.gz\"\n",
    "    },\n",
    "    ExecutionRoleArn=config.SAGEMAKER_IAM_ROLE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create endpoint configuration with auto-scaling\n",
    "endpoint_config_name = f\"fraud-detection-config-{int(time.time())}\"\n",
    "endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'VariantName': 'default',\n",
    "        'ModelName': model_name,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InstanceType': 'ml.t2.medium',\n",
    "        'InitialVariantWeight': 1\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Create endpoint\n",
    "endpoint_name = f\"{config.SOLUTION_PREFIX}-xgb\"\n",
    "endpoint = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Endpoint {endpoint_name} creation initiated\")\n",
    "\n",
    "# Wait for endpoint to become available\n",
    "print(\"Waiting for endpoint to be in service...\")\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "print(f\"Endpoint {endpoint_name} is now in service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predictor\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=CSVSerializer()\n",
    ")\n",
    "\n",
    "# Test on the test set\n",
    "def format_features_for_prediction(row):\n",
    "    \"\"\"Format a row of features for prediction\"\"\"\n",
    "    features_list = []\n",
    "    for feature in features:\n",
    "        features_list.append(str(row[feature]))\n",
    "    return ','.join(features_list)\n",
    "\n",
    "# Get predictions for test data\n",
    "print(\"Getting predictions for test data...\")\n",
    "y_pred_proba = []\n",
    "batch_size = 100  # Process in batches to avoid throttling\n",
    "\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    batch = X_test.iloc[i:i+batch_size]\n",
    "    batch_features = [format_features_for_prediction(row) for _, row in batch.iterrows()]\n",
    "    \n",
    "    # Send each row separately to avoid CSV parsing issues\n",
    "    batch_predictions = []\n",
    "    for features_str in batch_features:\n",
    "        response = predictor.predict(features_str)\n",
    "        pred = float(response.decode('utf-8'))\n",
    "        batch_predictions.append(pred)\n",
    "    \n",
    "    y_pred_proba.extend(batch_predictions)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Model Evaluation:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Save model details for reference\n",
    "model_details = {\n",
    "    'endpoint_name': endpoint_name,\n",
    "    'model_name': model_name,\n",
    "    'features': features,\n",
    "    'performance': {\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to S3\n",
    "model_details_key = f'{prefix}/model-details.json'\n",
    "s3.Bucket(bucket).Object(model_details_key).put(\n",
    "    Body=json.dumps(model_details, indent=2)\n",
    ")\n",
    "print(f\"Model details saved to s3://{bucket}/{model_details_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Transactions Generator for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_transactions(num_transactions=10, fraud_ratio=0.3):\n",
    "    \"\"\"Generate test transactions for demonstrating the model\"\"\"\n",
    "    from package.generator import generate_transaction\n",
    "    \n",
    "    test_transactions = []\n",
    "    \n",
    "    # Generate mostly legitimate transactions\n",
    "    for _ in range(int(num_transactions * (1 - fraud_ratio))):\n",
    "        transaction = generate_transaction(fraud_probability=0)\n",
    "        test_transactions.append(transaction)\n",
    "    \n",
    "    # Generate some fraudulent transactions\n",
    "    for _ in range(int(num_transactions * fraud_ratio)):\n",
    "        transaction = generate_transaction(fraud_probability=1)\n",
    "        test_transactions.append(transaction)\n",
    "    \n",
    "    return test_transactions\n",
    "\n",
    "# Generate test transactions\n",
    "test_transactions = generate_test_transactions(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format transactions for prediction\n",
    "def format_transaction_for_prediction(transaction):\n",
    "    \"\"\"Format a transaction for model prediction\"\"\"\n",
    "    # Extract features from the transaction\n",
    "    features_dict = {}\n",
    "    \n",
    "    # Basic features\n",
    "    features_dict['amount'] = float(transaction.get('amount', 0))\n",
    "    features_dict['is_vpn'] = 1 if transaction.get('is_vpn', False) else 0\n",
    "    \n",
    "    # Time-based features\n",
    "    import datetime\n",
    "    try:\n",
    "        timestamp = datetime.datetime.strptime(\n",
    "            transaction.get('timestamp', datetime.datetime.now().isoformat()),\n",
    "            '%Y-%m-%dT%H:%M:%SZ'\n",
    "        )\n",
    "        features_dict['hour_of_day'] = timestamp.hour\n",
    "        features_dict['day_of_week'] = timestamp.weekday()\n",
    "        features_dict['is_weekend'] = 1 if timestamp.weekday() >= 5 else 0\n",
    "        features_dict['is_night'] = 1 if (timestamp.hour < 6 or timestamp.hour >= 22) else 0\n",
    "    except:\n",
    "        features_dict['hour_of_day'] = 0\n",
    "        features_dict['day_of_week'] = 0\n",
    "        features_dict['is_weekend'] = 0\n",
    "        features_dict['is_night'] = 0\n",
    "    \n",
    "    # Location risk\n",
    "    location_risk = {\n",
    "        'California, USA': 0.2,\n",
    "        'New York, USA': 0.2,\n",
    "        'Texas, USA': 0.2,\n",
    "        'Florida, USA': 0.3,\n",
    "        'Illinois, USA': 0.2,\n",
    "        'London, UK': 0.3,\n",
    "        'Paris, France': 0.4,\n",
    "        'Berlin, Germany': 0.4,\n",
    "        'Tokyo, Japan': 0.5,\n",
    "        'Sydney, Australia': 0.5,\n",
    "        'Unknown': 0.9\n",
    "    }\n",
    "    features_dict['location_risk'] = location_risk.get(transaction.get('location', 'Unknown'), 0.7)\n",
    "    \n",
    "    # Mock user behavior features (would come from historical data in production)\n",
    "    features_dict['user_transaction_count'] = 5  # Dummy value\n",
    "    features_dict['amount_zscore'] = (features_dict['amount'] - 200) / 100  # Dummy calculation\n",
    "    \n",
    "    # Transaction risk score\n",
    "    features_dict['transaction_risk_score'] = (\n",
    "        features_dict['amount_zscore'] * 0.3 +\n",
    "        features_dict['location_risk'] * 0.2 +\n",
    "        features_dict['is_vpn'] * 0.2 +\n",
    "        features_dict['is_night'] * 0.1\n",
    "    )\n",
    "    \n",
    "    # One-hot encoded categorical features\n",
    "    device_type = transaction.get('device_type', 'unknown')\n",
    "    features_dict[f'devicetype{device_type}'] = 1\n",
    "    \n",
    "    card_type = transaction.get('card_type', 'unknown')\n",
    "    features_dict[f'card_type_{card_type}'] = 1\n",
    "    \n",
    "    status = transaction.get('status', 'approved')\n",
    "    features_dict[f'status_{status}'] = 1\n",
    "    \n",
    "    # Assemble feature string in the correct order\n",
    "    feature_values = []\n",
    "    for feature in features:\n",
    "        feature_values.append(str(features_dict.get(feature, 0)))\n",
    "    \n",
    "    return ','.join(feature_values)\n",
    "\n",
    "# Test the transactions against the model\n",
    "print(\"Testing transactions against the model...\")\n",
    "for transaction in test_transactions:\n",
    "    features_str = format_transaction_for_prediction(transaction)\n",
    "    \n",
    "    # Get prediction\n",
    "    response = predictor.predict(features_str)\n",
    "    fraud_probability = float(response.decode('utf-8'))\n",
    "    is_fraud = fraud_probability >= 0.5\n",
    "    \n",
    "    print(f\"Transaction {transaction['transaction_id']}:\")\n",
    "    print(f\"Amount: ${transaction['amount']:.2f}, Location: {transaction['location']}\")\n",
    "    print(f\"Device: {transaction['device_type']}, VPN: {transaction['is_vpn']}\")\n",
    "    print(f\"Prediction: {'FRAUD' if is_fraud else 'LEGITIMATE'} (probability: {fraud_probability:.4f})\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinesis Integration for Real-time Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_transactions_to_kinesis(transactions, stream_name=config.KINESIS_STREAM_NAME):\n",
    "    \"\"\"Send transactions to Kinesis for real-time processing\"\"\"\n",
    "    kinesis = boto3.client('kinesis')\n",
    "    \n",
    "    for transaction in transactions:\n",
    "        # Convert transaction to JSON\n",
    "        transaction_json = json.dumps(transaction)\n",
    "        \n",
    "        # Send to Kinesis\n",
    "        response = kinesis.put_record(\n",
    "            StreamName=stream_name,\n",
    "            Data=transaction_json,\n",
    "            PartitionKey=transaction['transaction_id']\n",
    "        )\n",
    "        \n",
    "        print(f\"Sent transaction {transaction['transaction_id']} to Kinesis: Shard {response['ShardId']}\")\n",
    "        \n",
    "        # Small delay to avoid throttling\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Generate a batch of transactions and send to Kinesis\n",
    "kinesis_test_transactions = generate_test_transactions(10, fraud_ratio=0.2)\n",
    "send_transactions_to_kinesis(kinesis_test_transactions)\n",
    "\n",
    "print(f\"Sent {len(kinesis_test_transactions)} transactions to Kinesis stream {config.KINESIS_STREAM_NAME}\")\n",
    "print(\"These will be processed by the Lambda function and results stored in DynamoDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard for Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dynamodb_for_results(transaction_ids, table_name=config.DYNAMODB_TABLE):\n",
    "    \"\"\"Check DynamoDB for processing results of test transactions\"\"\"\n",
    "    dynamodb = boto3.resource('dynamodb')\n",
    "    table = dynamodb.Table(table_name)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for tx_id in transaction_ids:\n",
    "        try:\n",
    "            response = table.get_item(Key={'transaction_id': tx_id})\n",
    "            if 'Item' in response:\n",
    "                results.append(response['Item'])\n",
    "            else:\n",
    "                print(f\"Transaction {tx_id} not found in DynamoDB yet.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving transaction {tx_id}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Give Lambda some time to process\n",
    "print(\"Waiting for Lambda to process transactions...\")\n",
    "time.sleep(10)  # Wait 10 seconds\n",
    "\n",
    "# Check DynamoDB for results\n",
    "tx_ids = [tx['transaction_id'] for tx in kinesis_test_transactions]\n",
    "results = check_dynamodb_for_results(tx_ids)\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"Found {len(results)} transactions in DynamoDB\")\n",
    "    \n",
    "    # Plot fraud probabilities\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='transaction_id', y='fraud_probability', data=results_df)\n",
    "    plt.title('Fraud Probabilities for Test Transactions')\n",
    "    plt.xlabel('Transaction ID')\n",
    "    plt.ylabel('Fraud Probability')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results found in DynamoDB yet. Lambda might still be processing or there might be an issue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explanation and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_fraud_prediction(transaction, fraud_probability):\n",
    "    \"\"\"Explain why a transaction is flagged as fraudulent\"\"\"\n",
    "    risk_factors = []\n",
    "    \n",
    "    # Check for high amount\n",
    "    if transaction['amount'] > 500:\n",
    "        risk_factors.append(f\"High transaction amount: ${transaction['amount']:.2f}\")\n",
    "    \n",
    "    # Check for VPN usage\n",
    "    if transaction.get('is_vpn', False):\n",
    "        risk_factors.append(\"VPN usage detected\")\n",
    "    \n",
    "    # Check for unusual location\n",
    "    location = transaction.get('location', '')\n",
    "    if location in ['Tokyo, Japan', 'Berlin, Germany', 'Paris, France', 'Sydney, Australia']:\n",
    "        risk_factors.append(f\"Transaction from unusual location: {location}\")\n",
    "    \n",
    "    # Check for gift card\n",
    "    if transaction.get('card_type', '') == 'gift':\n",
    "        risk_factors.append(\"Gift card usage\")\n",
    "    \n",
    "    # Check for mobile device\n",
    "    if transaction.get('device_type', '') == 'mobile':\n",
    "        risk_factors.append(\"Mobile device used\")\n",
    "    \n",
    "    # Check for declined status\n",
    "    if transaction.get('status', '') == 'declined':\n",
    "        risk_factors.append(\"Transaction was initially declined\")\n",
    "    \n",
    "    # Check for night-time transaction\n",
    "    try:\n",
    "        timestamp = datetime.datetime.strptime(\n",
    "            transaction.get('timestamp', ''),\n",
    "            '%Y-%m-%dT%H:%M:%SZ'\n",
    "        )\n",
    "        hour = timestamp.hour\n",
    "        if hour < 6 or hour >= 22:\n",
    "            risk_factors.append(f\"Transaction occurred during night hours: {hour}:00\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'transaction_id': transaction['transaction_id'],\n",
    "        'fraud_probability': fraud_probability,\n",
    "        'risk_score': fraud_probability,\n",
    "        'is_fraud': fraud_probability > 0.5,\n",
    "        'risk_factors': risk_factors,\n",
    "        'explanation': f\"Transaction flagged with {fraud_probability:.1%} fraud probability due to {len(risk_factors)} risk factors.\"\n",
    "    }\n",
    "\n",
    "# Demonstrate explanation for a high-risk transaction\n",
    "high_risk_transactions = [tx for tx in test_transactions if tx.get('is_vpn', False) and tx.get('amount', 0) > 500]\n",
    "if high_risk_transactions:\n",
    "    sample_tx = high_risk_transactions[0]\n",
    "    features_str = format_transaction_for_prediction(sample_tx)\n",
    "    \n",
    "    # Get prediction\n",
    "    response = predictor.predict(features_str)\n",
    "    fraud_probability = float(response.decode('utf-8'))\n",
    "    \n",
    "    # Get explanation\n",
    "    explanation = explain_fraud_prediction(sample_tx, fraud_probability)\n",
    "    \n",
    "    print(\"Sample Fraud Explanation:\")\n",
    "    print(f\"Transaction: {sample_tx['transaction_id']}\")\n",
    "    print(f\"Amount: ${sample_tx['amount']:.2f}, Location: {sample_tx['location']}\")\n",
    "    print(f\"Fraud Probability: {fraud_probability:.2%}\")\n",
    "    print(\"Risk Factors:\")\n",
    "    for factor in explanation['risk_factors']:\n",
    "        print(f\"- {factor}\")\n",
    "    print(f\"Explanation: {explanation['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the Fraud Detection Pipeline\n",
    "print(\"\"\"\n",
    "# Fraud Detection Pipeline Summary\n",
    "\n",
    "This notebook has established a complete fraud detection pipeline including:\n",
    "\n",
    "1. **Data Generation**: Created realistic transaction data with fraud patterns\n",
    "2. **Feature Engineering**: Transformed raw transaction data into predictive features\n",
    "3. **Model Training**: Trained and tuned an XGBoost model for fraud detection\n",
    "4. **Model Deployment**: Deployed the model to a SageMaker endpoint for real-time inference\n",
    "5. **Real-time Processing**: Connected to Kinesis for real-time transaction processing\n",
    "6. **Data Storage**: Stored processed transactions in DynamoDB\n",
    "7. **Monitoring**: Created visualizations for model performance and transaction processing\n",
    "\n",
    "## Production Considerations:\n",
    "\n",
    "1. **Monitoring**: Implement CloudWatch dashboards to monitor endpoint performance and fraud rates\n",
    "2. **Retraining**: Set up automatic retraining as new transaction data becomes available\n",
    "3. **Alerting**: Enhance the SNS notification system for high-value fraudulent transactions\n",
    "4. **Scaling**: Configure auto-scaling for the endpoint as transaction volume increases\n",
    "5. **Cost Optimization**: Move to SageMaker serverless inference for cost efficiency\n",
    "\n",
    "## Security Considerations:\n",
    "\n",
    "1. **Data Encryption**: Ensure all data is encrypted at rest and in transit\n",
    "2. **Access Control**: Implement strict IAM policies for accessing sensitive data\n",
    "3. **Audit Logging**: Enable comprehensive logging of all fraud detection activities\n",
    "4. **PII Handling**: Establish proper handling of personally identifiable information\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

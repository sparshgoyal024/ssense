{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-commerce Fraud Detection using SageMaker\n",
    "\n",
    "This notebook demonstrates how to build a machine learning model to detect fraudulent e-commerce transactions. We'll use SageMaker to train and deploy the model, then test it on simulated transaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import io\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('./src/'))\n",
    "from package import config\n",
    "\n",
    "# Initialize AWS clients\n",
    "session = sagemaker.Session()\n",
    "s3 = boto3.resource('s3')\n",
    "sm_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import config variables\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('./src/'))\n",
    "from package import config\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the stack outputs from the file\n",
    "with open('/home/ec2-user/SageMaker/stack_outputs_processed.json', 'r') as f:\n",
    "    stack_outputs = json.load(f)\n",
    "\n",
    "SAGEMAKER_IAM_ROLE = stack_outputs.get('IamRole')\n",
    "print(f\"Using IAM Role: {SAGEMAKER_IAM_ROLE}\")\n",
    "bucket = stack_outputs.get('ModelDataBucket')\n",
    "print(f\"bucekt: {bucket}\")\n",
    "SOLUTION_PREFIX = stack_outputs.get('SolutionPrefix')\n",
    "\n",
    "\n",
    "\n",
    "# Get other variables you need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Enhance the existing data generation with more fraud patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def load_transaction_data_from_s3(bucket_name=None, prefix='historical-data', filename='transactions.json'):\n",
    "    # Get bucket name from environment variable if not provided\n",
    "    bucket_name = bucket\n",
    "    \n",
    "    # Initialize S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Full S3 key\n",
    "    s3_key = f\"{prefix}/{filename}\"\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Loading transaction data from s3://{bucket_name}/{s3_key}\")\n",
    "        \n",
    "        # Get object from S3\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "        \n",
    "        # Read JSON content\n",
    "        json_content = response['Body'].read().decode('utf-8')\n",
    "        transactions = json.loads(json_content)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(transactions)\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {len(df)} transactions from S3\")\n",
    "        logger.info(f\"Fraud percentage: {df['is_fraud'].mean() * 100:.2f}%\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading transaction data from S3: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "df = load_transaction_data_from_s3()\n",
    "df.head()\n",
    "\n",
    "# To use in a SageMaker notebook:\n",
    "def load_data_for_training(bucket_name=None):\n",
    "    \"\"\"\n",
    "    Load transaction data from S3 and prepare it for model training\n",
    "    \"\"\"\n",
    "    # Load data from S3\n",
    "    df = load_transaction_data_from_s3(bucket_name)\n",
    "    \n",
    "    # Display info about the loaded data\n",
    "    print(f\"Loaded {len(df)} transactions from S3\")\n",
    "    print(f\"Fraud percentage: {df['is_fraud'].mean() * 100:.2f}%\")\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Feature Engineering\n",
    "\n",
    "Add more sophisticated feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def engineer_features_for_ml(df):\n",
    "    \"\"\"\n",
    "    Feature engineering for ML-based fraud detection.\n",
    "    Uses the ground truth fraud labels from the data rather than creating rule-based labels.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Ensure we have a fraud label column from the data\n",
    "    if 'is_fraud' not in df_features.columns:\n",
    "        raise ValueError(\"Data must contain an 'is_fraud' column with ground truth labels\")\n",
    "    \n",
    "    # Convert timestamp to datetime if it's not already\n",
    "    if not pd.api.types.is_datetime64_dtype(df_features['timestamp']):\n",
    "        df_features['timestamp'] = pd.to_datetime(df_features['timestamp'])\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df_features['hour_of_day'] = df_features['timestamp'].dt.hour\n",
    "    df_features['day_of_week'] = df_features['timestamp'].dt.dayofweek\n",
    "    df_features['month'] = df_features['timestamp'].dt.month\n",
    "    df_features['day_of_month'] = df_features['timestamp'].dt.day\n",
    "    df_features['is_weekend'] = df_features['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df_features['is_night'] = df_features['hour_of_day'].apply(lambda x: 1 if (x < 6 or x >= 22) else 0)\n",
    "    \n",
    "    # User behavior features - calculated properly for both training and prediction\n",
    "    # Group by user_id to get transaction counts and statistics\n",
    "    user_stats = df_features.groupby('user_id').agg({\n",
    "        'transaction_id': 'count',\n",
    "        'amount': ['mean', 'std', 'min', 'max'],\n",
    "        'is_vpn': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Flatten multi-level columns\n",
    "    user_stats.columns = ['_'.join(col).strip() for col in user_stats.columns.values]\n",
    "    user_stats.rename(columns={\n",
    "        'transaction_id_count': 'user_transaction_count',\n",
    "        'amount_mean': 'user_avg_amount',\n",
    "        'amount_std': 'user_amount_std',\n",
    "        'amount_min': 'user_min_amount',\n",
    "        'amount_max': 'user_max_amount',\n",
    "        'is_vpn_mean': 'user_vpn_ratio'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Handle users with only one transaction (no std)\n",
    "    user_stats['user_amount_std'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Reset index to make user_id a column again\n",
    "    user_stats.reset_index(inplace=True)\n",
    "    \n",
    "    # Merge user statistics back to the main dataframe\n",
    "    df_features = df_features.merge(user_stats, on='user_id', how='left')\n",
    "    \n",
    "    # Calculate transaction amount z-score relative to user's history\n",
    "    # Use a safe calculation that handles division by zero\n",
    "    df_features['amount_zscore'] = df_features.apply(\n",
    "        lambda row: (row['amount'] - row['user_avg_amount']) / (row['user_amount_std'] if row['user_amount_std'] > 0 else 1),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create features for relative transaction size\n",
    "    df_features['amount_to_max_ratio'] = df_features.apply(\n",
    "        lambda row: row['amount'] / row['user_max_amount'] if row['user_max_amount'] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Location features - use past fraud rates instead of hardcoded risk\n",
    "    location_fraud_rates = df_features.groupby('location')['is_fraud'].mean().to_dict()\n",
    "    df_features['location_fraud_rate'] = df_features['location'].map(location_fraud_rates)\n",
    "    \n",
    "    # Handle NaN values for new locations\n",
    "    df_features['location_fraud_rate'].fillna(df_features['is_fraud'].mean(), inplace=True)\n",
    "    \n",
    "    # Device and card type features\n",
    "    device_fraud_rates = df_features.groupby('device_type')['is_fraud'].mean().to_dict()\n",
    "    df_features['device_fraud_rate'] = df_features['device_type'].map(device_fraud_rates)\n",
    "    \n",
    "    card_fraud_rates = df_features.groupby('card_type')['is_fraud'].mean().to_dict()\n",
    "    df_features['card_fraud_rate'] = df_features['card_type'].map(card_fraud_rates)\n",
    "    \n",
    "    # Convert boolean to integer if needed\n",
    "    if pd.api.types.is_bool_dtype(df_features['is_vpn']):\n",
    "        df_features['is_vpn'] = df_features['is_vpn'].astype(int)\n",
    "    \n",
    "    # Create dummy variables for categorical features\n",
    "    categorical_features = ['device_type', 'card_type', 'status', 'location']\n",
    "    df_features = pd.get_dummies(df_features, columns=categorical_features, drop_first=False)\n",
    "    \n",
    "    # New feature: is this amount unusual for this user?\n",
    "    df_features['is_unusual_amount'] = (\n",
    "        (df_features['amount'] > (df_features['user_avg_amount'] + 2 * df_features['user_amount_std'])) |\n",
    "        (df_features['amount'] < (df_features['user_avg_amount'] - 2 * df_features['user_amount_std']))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # New feature: is this a new user (fewer than N transactions)?\n",
    "    df_features['is_new_user'] = (df_features['user_transaction_count'] <= 3).astype(int)\n",
    "    \n",
    "    # New feature: is this the user's largest transaction?\n",
    "    df_features['is_largest_tx'] = (df_features['amount'] >= df_features['user_max_amount'] * 0.95).astype(int)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "def create_ml_preprocessing_pipeline(df_features):\n",
    "    \"\"\"\n",
    "    Create a scikit-learn preprocessing pipeline for ML model training.\n",
    "    This ensures consistent feature transformation for training and prediction.\n",
    "    \"\"\"\n",
    "    # List your columns by type\n",
    "    numeric_features = [\n",
    "        'amount', 'hour_of_day', 'day_of_week', 'month', 'day_of_month', \n",
    "        'is_weekend', 'is_night', 'is_vpn', 'user_transaction_count',\n",
    "        'user_avg_amount', 'user_amount_std', 'user_min_amount', 'user_max_amount',\n",
    "        'user_vpn_ratio', 'amount_zscore', 'amount_to_max_ratio', \n",
    "        'location_fraud_rate', 'device_fraud_rate', 'card_fraud_rate',\n",
    "        'is_unusual_amount', 'is_new_user', 'is_largest_tx'\n",
    "    ]\n",
    "    \n",
    "    # Create column transformer for preprocessing\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "        ],\n",
    "        remainder='passthrough'  # This keeps the dummy variables without scaling\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def train_test_split_with_features(df):\n",
    "    \"\"\"\n",
    "    Load data, engineer features, and prepare train/test splits with proper feature preprocessing\n",
    "    \"\"\"\n",
    "    # Apply feature engineering\n",
    "    df_features = engineer_features_for_ml(df)\n",
    "    \n",
    "    # Display feature correlations with fraud\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    # Calculate correlation only for numeric columns\n",
    "    numeric_df = df_features.select_dtypes(include=['number'])\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "    fraud_correlations = correlation_matrix['is_fraud'].sort_values(ascending=False)\n",
    "    print(\"Top features correlated with fraud:\")\n",
    "    print(fraud_correlations.head(15))\n",
    "\n",
    "    # Plot correlation heatmap for top correlated features\n",
    "    top_corr_features = fraud_correlations.index[:15]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        correlation_matrix.loc[top_corr_features, top_corr_features], \n",
    "        annot=True, \n",
    "        cmap='coolwarm'\n",
    "    )\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the engineered features dataframe\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Before preprocessing, check for duplicate columns\n",
    "\n",
    "\n",
    "def engineer_features_for_ml(df):\n",
    "    \"\"\"\n",
    "    Feature engineering for ML-based fraud detection.\n",
    "    Uses the ground truth fraud labels from the data rather than creating rule-based labels.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Ensure we have a fraud label column from the data\n",
    "    if 'is_fraud' not in df_features.columns:\n",
    "        raise ValueError(\"Data must contain an 'is_fraud' column with ground truth labels\")\n",
    "    \n",
    "    # Convert timestamp to datetime if it's not already\n",
    "    if not pd.api.types.is_datetime64_dtype(df_features['timestamp']):\n",
    "        df_features['timestamp'] = pd.to_datetime(df_features['timestamp'])\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df_features['hour_of_day'] = df_features['timestamp'].dt.hour\n",
    "    df_features['day_of_week'] = df_features['timestamp'].dt.dayofweek\n",
    "    df_features['month'] = df_features['timestamp'].dt.month\n",
    "    df_features['day_of_month'] = df_features['timestamp'].dt.day\n",
    "    df_features['is_weekend'] = df_features['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df_features['is_night'] = df_features['hour_of_day'].apply(lambda x: 1 if (x < 6 or x >= 22) else 0)\n",
    "    \n",
    "    # User behavior features - calculated properly for both training and prediction\n",
    "    # Group by user_id to get transaction counts and statistics\n",
    "    user_stats = df_features.groupby('user_id').agg({\n",
    "        'transaction_id': 'count',\n",
    "        'amount': ['mean', 'std', 'max'],\n",
    "        'is_vpn': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Flatten multi-level columns\n",
    "    user_stats.columns = ['_'.join(col).strip() for col in user_stats.columns.values]\n",
    "    user_stats.rename(columns={\n",
    "        'transaction_id_count': 'user_transaction_count',\n",
    "        'amount_mean': 'user_avg_amount',\n",
    "        'amount_std': 'user_amount_std',\n",
    "        'amount_max': 'user_max_amount',\n",
    "        'is_vpn_mean': 'user_vpn_ratio'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Handle users with only one transaction (no std)\n",
    "    user_stats['user_amount_std'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Reset index to make user_id a column again\n",
    "    user_stats.reset_index(inplace=True)\n",
    "    \n",
    "    # Merge user statistics back to the main dataframe\n",
    "    df_features = df_features.merge(user_stats, on='user_id', how='left')\n",
    "    \n",
    "    # Calculate transaction amount z-score relative to user's history\n",
    "    # Use a safe calculation that handles division by zero\n",
    "    df_features['amount_zscore'] = df_features.apply(\n",
    "        lambda row: (row['amount'] - row['user_avg_amount']) / (row['user_amount_std'] if row['user_amount_std'] > 0 else 1),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create features for relative transaction size\n",
    "    df_features['amount_to_max_ratio'] = df_features.apply(\n",
    "        lambda row: row['amount'] / row['user_max_amount'] if row['user_max_amount'] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Location features - use past fraud rates instead of hardcoded risk\n",
    "    location_fraud_rates = df_features.groupby('location')['is_fraud'].mean().to_dict()\n",
    "    df_features['location_fraud_rate'] = df_features['location'].map(location_fraud_rates)\n",
    "    \n",
    "    # Handle NaN values for new locations\n",
    "    df_features['location_fraud_rate'].fillna(df_features['is_fraud'].mean(), inplace=True)\n",
    "    \n",
    "    # Device and card type features\n",
    "    device_fraud_rates = df_features.groupby('device_type')['is_fraud'].mean().to_dict()\n",
    "    df_features['device_fraud_rate'] = df_features['device_type'].map(device_fraud_rates)\n",
    "    \n",
    "    card_fraud_rates = df_features.groupby('card_type')['is_fraud'].mean().to_dict()\n",
    "    df_features['card_fraud_rate'] = df_features['card_type'].map(card_fraud_rates)\n",
    "    \n",
    "    # Convert boolean to integer if needed\n",
    "    if pd.api.types.is_bool_dtype(df_features['is_vpn']):\n",
    "        df_features['is_vpn'] = df_features['is_vpn'].astype(int)\n",
    "    \n",
    "    # Create dummy variables for categorical features\n",
    "    categorical_features = ['device_type', 'card_type', 'status', 'location']\n",
    "    df_features = pd.get_dummies(df_features, columns=categorical_features, drop_first=False)\n",
    "    \n",
    "    # New feature: is this amount unusual for this user?\n",
    "    df_features['is_unusual_amount'] = (\n",
    "        (df_features['amount'] > (df_features['user_avg_amount'] + 2 * df_features['user_amount_std'])) |\n",
    "        (df_features['amount'] < (df_features['user_avg_amount'] - 2 * df_features['user_amount_std']))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # New feature: is this a new user (fewer than N transactions)?\n",
    "    df_features['is_new_user'] = (df_features['user_transaction_count'] <= 3).astype(int)\n",
    "    \n",
    "    # New feature: is this the user's largest transaction?\n",
    "    df_features['is_largest_tx'] = (df_features['amount'] >= df_features['user_max_amount'] * 0.95).astype(int)\n",
    "    \n",
    "    # Check for duplicate columns\n",
    "    if df_features.columns.duplicated().any():\n",
    "        print(\"Warning: Found duplicate columns after feature engineering.\")\n",
    "        # Get the duplicated column names\n",
    "        duplicated_cols = df_features.columns[df_features.columns.duplicated()].tolist()\n",
    "        print(f\"Duplicate columns: {duplicated_cols}\")\n",
    "        \n",
    "        # Drop duplicates\n",
    "        df_features = df_features.loc[:, ~df_features.columns.duplicated()]\n",
    "        print(f\"Removed duplicate columns. New shape: {df_features.shape}\")\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "# Fix for the create_ml_preprocessing_pipeline function\n",
    "def create_ml_preprocessing_pipeline(df_features, X=None):\n",
    "    \"\"\"\n",
    "    Create a scikit-learn preprocessing pipeline for ML model training.\n",
    "    This ensures consistent feature transformation for training and prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    df_features: DataFrame with all features\n",
    "    X: The actual X_train or X data to be processed (optional)\n",
    "    \"\"\"\n",
    "    # Define numeric features to use\n",
    "    numeric_features = [\n",
    "        'amount', 'hour_of_day', 'day_of_week', 'month', 'day_of_month', \n",
    "        'is_weekend', 'is_night', 'is_vpn', 'user_transaction_count',\n",
    "        'user_avg_amount', 'user_amount_std', 'amount_zscore', \n",
    "        'amount_to_max_ratio', 'location_fraud_rate', 'device_fraud_rate', \n",
    "        'card_fraud_rate', 'is_unusual_amount', 'is_new_user', 'is_largest_tx'\n",
    "    ]\n",
    "    \n",
    "    # If X is provided, check for duplicates in X\n",
    "    if X is not None:\n",
    "        # Check for duplicate columns in X\n",
    "        if X.columns.duplicated().any():\n",
    "            dupe_cols = X.columns[X.columns.duplicated()].tolist()\n",
    "            print(f\"Warning: Found duplicate columns in X: {dupe_cols}\")\n",
    "            \n",
    "            # Find first occurrence of each column name\n",
    "            keep_cols = ~X.columns.duplicated(keep='first')\n",
    "            X_unique = X.loc[:, keep_cols]\n",
    "            print(f\"Removed duplicate columns. X shape before: {X.shape}, after: {X_unique.shape}\")\n",
    "            \n",
    "            # Update the provided X dataframe in-place with unique columns\n",
    "            # Note: This is modifying the input dataframe\n",
    "            # This is a bit of a hack but should work for your case\n",
    "            X = X_unique\n",
    "    \n",
    "    # Ensure we only use features that exist in the dataframe\n",
    "    available_numeric_features = [col for col in numeric_features if col in df_features.columns]\n",
    "    \n",
    "    print(f\"Using these numeric features for preprocessing: {available_numeric_features}\")\n",
    "    \n",
    "    # Create column transformer for preprocessing\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), available_numeric_features),\n",
    "        ],\n",
    "        remainder='passthrough'  # This keeps the dummy variables without scaling\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def train_test_split_with_features(df):\n",
    "    \"\"\"\n",
    "    Load data, engineer features, and prepare train/test splits with proper feature preprocessing\n",
    "    \"\"\"\n",
    "    # Apply feature engineering\n",
    "    df_features = engineer_features_for_ml(df)\n",
    "    \n",
    "    # Define numeric features to use for the model\n",
    "    numeric_features = [\n",
    "        'amount', 'hour_of_day', 'day_of_week', 'month', 'day_of_month',\n",
    "        'is_weekend', 'is_night', 'is_vpn', 'user_transaction_count',\n",
    "        'user_avg_amount', 'user_amount_std', 'amount_zscore', \n",
    "        'amount_to_max_ratio', 'location_fraud_rate', 'device_fraud_rate', \n",
    "        'card_fraud_rate', 'is_unusual_amount', 'is_new_user', 'is_largest_tx'\n",
    "    ]\n",
    "    \n",
    "    # Get all one-hot encoded categorical columns\n",
    "    categorical_columns = [col for col in df_features.columns if \n",
    "                          col.startswith('device_type_') or \n",
    "                          col.startswith('card_type_') or \n",
    "                          col.startswith('status_') or\n",
    "                          col.startswith('location_')]\n",
    "    \n",
    "    # Combine all features\n",
    "    features = numeric_features + categorical_columns\n",
    "    \n",
    "    # Define target\n",
    "    target = 'is_fraud'\n",
    "    \n",
    "    # Check if all features exist in the dataframe\n",
    "    missing_features = [f for f in features if f not in df_features.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Warning: These features are missing from the dataframe: {missing_features}\")\n",
    "        # Keep only features that exist in the dataframe\n",
    "        features = [f for f in features if f in df_features.columns]\n",
    "    \n",
    "    # Check for duplicate columns in the dataframe\n",
    "    duplicates = df_features.columns[df_features.columns.duplicated()].tolist()\n",
    "    if duplicates:\n",
    "        print(f\"Warning: Found duplicate columns: {duplicates}\")\n",
    "        # Drop duplicate columns\n",
    "        df_features = df_features.loc[:, ~df_features.columns.duplicated()]\n",
    "        print(f\"Removed duplicate columns. New shape: {df_features.shape}\")\n",
    "    \n",
    "    # Split the data\n",
    "    X = df_features[features]\n",
    "    y = df_features[target]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    print(f\"Fraud ratio in training: {y_train.mean():.2f}\")\n",
    "    print(f\"Fraud ratio in testing: {y_test.mean():.2f}\")\n",
    "    \n",
    "    return df_features, X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Training with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = load_data_for_training()\n",
    "\n",
    "# Apply feature engineering only once\n",
    "df_engineered = engineer_features_for_ml(df)\n",
    "\n",
    "# Define features\n",
    "numeric_features = [\n",
    "    'amount', 'hour_of_day', 'day_of_week', 'month', 'day_of_month',\n",
    "    'is_weekend', 'is_night', 'is_vpn', 'user_transaction_count',\n",
    "    'user_avg_amount', 'user_amount_std', 'amount_zscore', \n",
    "    'amount_to_max_ratio', 'location_fraud_rate', 'device_fraud_rate', \n",
    "    'card_fraud_rate', 'is_unusual_amount', 'is_new_user', 'is_largest_tx'\n",
    "]\n",
    "\n",
    "# Get categorical columns\n",
    "categorical_columns = [col for col in df_engineered.columns if \n",
    "                      col.startswith('device_type_') or \n",
    "                      col.startswith('card_type_') or \n",
    "                      col.startswith('status_') or\n",
    "                      col.startswith('location_')]\n",
    "\n",
    "# Combine all features\n",
    "features = numeric_features + categorical_columns\n",
    "\n",
    "# Define target\n",
    "target = 'is_fraud'\n",
    "\n",
    "# Check for duplicated columns in the engineered dataframe\n",
    "if df_engineered.columns.duplicated().any():\n",
    "    print(\"Warning: Found duplicate columns in engineered dataframe.\")\n",
    "    dupe_cols = df_engineered.columns[df_engineered.columns.duplicated()].tolist()\n",
    "    print(f\"Duplicate columns: {dupe_cols}\")\n",
    "    \n",
    "    # Remove duplicates, keeping only the first occurrence\n",
    "    df_engineered = df_engineered.loc[:, ~df_engineered.columns.duplicated()]\n",
    "    print(f\"Removed duplicate columns. Shape: {df_engineered.shape}\")\n",
    "\n",
    "# Remove features that don't exist\n",
    "features = [f for f in features if f in df_engineered.columns]\n",
    "\n",
    "# Split the data\n",
    "X = df_engineered[features]\n",
    "y = df_engineered[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(f\"Fraud ratio in training: {y_train.mean():.2f}\")\n",
    "print(f\"Fraud ratio in testing: {y_test.mean():.2f}\")\n",
    "\n",
    "# Create preprocessing pipeline, passing both the feature dataframe and X_train\n",
    "preprocessor = create_ml_preprocessing_pipeline(df_engineered, X_train)\n",
    "\n",
    "# Remove duplicate columns from X_train and X_test if they exist\n",
    "if X_train.columns.duplicated().any():\n",
    "    X_train = X_train.loc[:, ~X_train.columns.duplicated()]\n",
    "    print(f\"Removed duplicate columns from X_train. New shape: {X_train.shape}\")\n",
    "\n",
    "if X_test.columns.duplicated().any():\n",
    "    X_test = X_test.loc[:, ~X_test.columns.duplicated()]\n",
    "    print(f\"Removed duplicate columns from X_test. New shape: {X_test.shape}\")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed training data shape: {X_train_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import os\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "def upload_training_data_to_s3(X_train, y_train, X_val=None, y_val=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Get S3 bucket from config\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = bucket\n",
    "    print(bucket)\n",
    "    prefix = 'fraud-classifier'\n",
    "    \n",
    "    # Create a buffer for the training data in SVM light format\n",
    "    train_file = io.BytesIO()\n",
    "    dump_svmlight_file(X_train, y_train, train_file)\n",
    "    train_file.seek(0)\n",
    "    \n",
    "    # Upload training data to S3\n",
    "    train_key = f'{prefix}/train/train.libsvm'\n",
    "    s3.Bucket(bucket).Object(train_key).upload_fileobj(train_file)\n",
    "    train_data_s3_uri = f's3://{bucket}/{train_key}'\n",
    "    print(f\"Uploaded training data to {train_data_s3_uri}\")\n",
    "    \n",
    "    # If validation data is provided, upload it too\n",
    "    val_data_s3_uri = None\n",
    "    if X_val is not None and y_val is not None:\n",
    "        val_file = io.BytesIO()\n",
    "        dump_svmlight_file(X_val, y_val, val_file)\n",
    "        val_file.seek(0)\n",
    "        \n",
    "        val_key = f'{prefix}/validation/validation.libsvm'\n",
    "        s3.Bucket(bucket).Object(val_key).upload_fileobj(val_file)\n",
    "        val_data_s3_uri = f's3://{bucket}/{val_key}'\n",
    "        print(f\"Uploaded validation data to {val_data_s3_uri}\")\n",
    "    \n",
    "    # Set output location\n",
    "    output_s3_uri = f's3://{bucket}/{prefix}/output'\n",
    "    \n",
    "    return {\n",
    "        'train_data_s3_uri': train_data_s3_uri,\n",
    "        'validation_data_s3_uri': val_data_s3_uri,\n",
    "        'output_s3_uri': output_s3_uri\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize S3 resource\n",
    "s3 = boto3.resource('s3')\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Create a further split of training data to get a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create train file in SVMlight format\n",
    "train_file = io.BytesIO()\n",
    "dump_svmlight_file(X_train, y_train, train_file)\n",
    "train_file.seek(0)\n",
    "\n",
    "# Create validation file in SVMlight format\n",
    "validation_file = io.BytesIO()\n",
    "dump_svmlight_file(X_val, y_val, validation_file)\n",
    "validation_file.seek(0)\n",
    "\n",
    "# Get the bucket name from environment or config\n",
    "bucket = bucket\n",
    "prefix = 'fraud-classifier'\n",
    "\n",
    "# Upload train data\n",
    "train_key = f'{prefix}/train/train.libsvm'\n",
    "s3.Bucket(bucket).Object(train_key).upload_fileobj(train_file)\n",
    "train_data_s3_uri = f's3://{bucket}/{train_key}'\n",
    "\n",
    "# Upload validation data\n",
    "val_key = f'{prefix}/validation/validation.libsvm'\n",
    "s3.Bucket(bucket).Object(val_key).upload_fileobj(validation_file)\n",
    "validation_data_s3_uri = f's3://{bucket}/{val_key}'\n",
    "\n",
    "# Set output location\n",
    "output_s3_uri = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "print(f\"Uploaded training data to {train_data_s3_uri}\")\n",
    "print(f\"Uploaded validation data to {validation_data_s3_uri}\")\n",
    "\n",
    "# Get the XGBoost image - use the newer SageMaker API\n",
    "container = image_uris.retrieve(\"xgboost\", boto3.Session().region_name, version=\"1.0-1\")\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.01, 0.3),\n",
    "    'gamma': ContinuousParameter(0, 5),\n",
    "    'min_child_weight': IntegerParameter(1, 10),\n",
    "    'subsample': ContinuousParameter(0.5, 1.0),\n",
    "    'colsample_bytree': ContinuousParameter(0.5, 1.0)\n",
    "}\n",
    "\n",
    "# Create an estimator with both train and validation channels\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=SAGEMAKER_IAM_ROLE,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    output_path=output_s3_uri,\n",
    "    sagemaker_session=session,\n",
    "    base_job_name='fraud-detection-xgb'\n",
    ")\n",
    "\n",
    "# Set static hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    num_round=100,\n",
    "    rate_drop=0.1,\n",
    "    scale_pos_weight=10,  # Helpful for imbalanced datasets\n",
    "    # Add the following for better handling of missing values and numerical stability\n",
    "    tree_method='auto',\n",
    "    max_delta_step=3,    # Helpful for unbalanced classes\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Create the tuner with the correct metric name\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb,\n",
    "    'validation:auc',  # Make sure this matches eval_metric\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=5,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type='Maximize'\n",
    ")\n",
    "\n",
    "# Start the hyperparameter tuning job with both train and validation\n",
    "tuner.fit({\n",
    "    'train': train_data_s3_uri,\n",
    "    'validation': validation_data_s3_uri\n",
    "})\n",
    "print(\"Hyperparameter tuning job started\")\n",
    "\n",
    "# Save feature information for later use with the model\n",
    "feature_info = {\n",
    "    'feature_names': list(X_train.columns),\n",
    "    'categorical_features': [col for col in X_train.columns if \n",
    "                             col.startswith('device_type_') or \n",
    "                             col.startswith('card_type_') or \n",
    "                             col.startswith('status_') or\n",
    "                             col.startswith('location_')],\n",
    "    'numeric_features': [col for col in X_train.columns if \n",
    "                         not (col.startswith('device_type_') or \n",
    "                              col.startswith('card_type_') or \n",
    "                              col.startswith('status_') or\n",
    "                              col.startswith('location_'))]\n",
    "}\n",
    "\n",
    "# Store feature information in S3 for deployment\n",
    "feature_info_key = f'{prefix}/model/feature_info.json'\n",
    "s3.Bucket(bucket).Object(feature_info_key).put(\n",
    "    Body=json.dumps(feature_info, indent=2)\n",
    ")\n",
    "print(f\"Saved feature information to s3://{bucket}/{feature_info_key}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Deployment and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize SageMaker client\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# Get the best model from hyperparameter tuning\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "best_job_name = sm_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")['BestTrainingJob']['TrainingJobName']\n",
    "\n",
    "print(f\"Best training job: {best_job_name}\")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hyperparameters = sm_client.describe_training_job(\n",
    "    TrainingJobName=best_job_name\n",
    ")['HyperParameters']\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "for param, value in best_hyperparameters.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Create model\n",
    "model_name = f\"fraud-detection-model-{int(time.time())}\"\n",
    "model_info = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': f\"{output_s3_uri}/{best_job_name}/output/model.tar.gz\"\n",
    "    },\n",
    "    ExecutionRoleArn=SAGEMAKER_IAM_ROLE\n",
    ")\n",
    "\n",
    "print(f\"Created model: {model_name}\")\n",
    "\n",
    "# Store model metadata including the feature information\n",
    "model_metadata = {\n",
    "    'model_name': model_name,\n",
    "    'training_job': best_job_name,\n",
    "    'hyperparameters': {k: v for k, v in best_hyperparameters.items() if not k.startswith('_')},\n",
    "    'creation_time': time.time(),\n",
    "    'feature_info': feature_info  # This comes from the previous step\n",
    "}\n",
    "\n",
    "# Save model metadata to S3\n",
    "metadata_key = f'{prefix}/models/{model_name}/metadata.json'\n",
    "s3.Bucket(bucket).Object(metadata_key).put(\n",
    "    Body=json.dumps(model_metadata, indent=2)\n",
    ")\n",
    "\n",
    "print(f\"Saved model metadata to s3://{bucket}/{metadata_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Initialize SageMaker client\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# Create endpoint configuration with auto-scaling\n",
    "endpoint_config_name = f\"fraud-detection-config-{int(time.time())}\"\n",
    "endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'VariantName': 'default',\n",
    "        'ModelName': model_name,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InstanceType': 'ml.t2.medium',\n",
    "        'InitialVariantWeight': 1\n",
    "    }]\n",
    ")\n",
    "\n",
    "logger.info(f\"Created endpoint configuration: {endpoint_config_name}\")\n",
    "\n",
    "# Create endpoint\n",
    "endpoint_name = f\"{SOLUTION_PREFIX}-xgb\"\n",
    "endpoint = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "logger.info(f\"Endpoint {endpoint_name} creation initiated\")\n",
    "\n",
    "# Wait for endpoint to become available\n",
    "logger.info(\"Waiting for endpoint to be in service...\")\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "logger.info(f\"Endpoint {endpoint_name} is now in service\")\n",
    "\n",
    "# Store endpoint information in a central registry\n",
    "endpoint_info = {\n",
    "    'endpoint_name': endpoint_name,\n",
    "    'model_name': model_name,\n",
    "    'creation_time': time.time(),\n",
    "    'instance_type': 'ml.t2.medium',\n",
    "    'instance_count': 1,\n",
    "    'endpoint_config': endpoint_config_name\n",
    "}\n",
    "\n",
    "# Save endpoint info to S3\n",
    "endpoint_info_key = f'{prefix}/endpoints/{endpoint_name}/info.json'\n",
    "s3.Bucket(bucket).Object(endpoint_info_key).put(\n",
    "    Body=json.dumps(endpoint_info, indent=2)\n",
    ")\n",
    "\n",
    "print(f\"Endpoint {endpoint_name} is ready for inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Create a predictor for the endpoint\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=CSVSerializer()\n",
    ")\n",
    "\n",
    "# Test on a sample of the test set (first 200 records)\n",
    "test_sample = X_test.head(10000)\n",
    "y_test_sample = y_test.iloc[:10000]\n",
    "\n",
    "# Print sample sizes\n",
    "print(f\"Using sample of {len(test_sample)} records for testing\")\n",
    "print(f\"Sample fraud ratio: {y_test_sample.mean():.2f}\")\n",
    "\n",
    "# Get the expected feature count by reading the feature info from S3\n",
    "def get_model_feature_count():\n",
    "    try:\n",
    "        # Try to get feature count from saved feature info\n",
    "        feature_info_key = f'{prefix}/model/feature_info.json'\n",
    "        response = s3.Bucket(bucket).Object(feature_info_key).get()\n",
    "        feature_info = json.loads(response['Body'].read().decode('utf-8'))\n",
    "        return len(feature_info.get('feature_names', []))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get feature info from S3: {str(e)}\")\n",
    "        return 38  # Fallback to known count from the error message\n",
    "\n",
    "# Get expected feature count for model\n",
    "model_feature_count = get_model_feature_count()\n",
    "print(f\"Model expects {model_feature_count} features\")\n",
    "\n",
    "# Format features for prediction - ensuring we match the expected count\n",
    "def format_features_for_prediction(row, feature_count=model_feature_count):\n",
    "    \"\"\"Format a row of features for prediction matching the expected feature count\"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    # Get the first feature_count features (or pad with zeros if not enough)\n",
    "    feature_keys = list(row.index)\n",
    "    \n",
    "    for i in range(feature_count):\n",
    "        if i < len(feature_keys):\n",
    "            feature = feature_keys[i]\n",
    "            features_list.append(str(row[feature]))\n",
    "        else:\n",
    "            features_list.append('0')  # Pad with zeros if needed\n",
    "    \n",
    "    return ','.join(features_list)\n",
    "\n",
    "# Get predictions for test data\n",
    "print(\"Getting predictions for test data...\")\n",
    "y_pred_proba = []\n",
    "batch_size = 100  # Process in smaller batches\n",
    "\n",
    "# Use tqdm for a progress bar\n",
    "for i in tqdm(range(0, len(test_sample), batch_size), desc=\"Processing batches\"):\n",
    "    batch = test_sample.iloc[i:i+batch_size]\n",
    "    batch_features = [format_features_for_prediction(row) for _, row in batch.iterrows()]\n",
    "    \n",
    "    # Send each row separately to avoid CSV parsing issues\n",
    "    batch_predictions = []\n",
    "    for features_str in batch_features:\n",
    "        try:\n",
    "            response = predictor.predict(features_str)\n",
    "            pred = float(response.decode('utf-8'))\n",
    "            batch_predictions.append(pred)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with prediction: {str(e)}\")\n",
    "            print(f\"Problematic features: {features_str[:100]}...\")  # Print first 100 chars\n",
    "            batch_predictions.append(0.5)  # Default fallback value\n",
    "    \n",
    "    y_pred_proba.extend(batch_predictions)\n",
    "    \n",
    "    # Small delay to avoid throttling\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred_proba]\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_sample, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test_sample, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test_sample, y_pred_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Save test results to S3\n",
    "test_results = {\n",
    "    'endpoint_name': endpoint_name,\n",
    "    'test_time': time.time(),\n",
    "    'metrics': {\n",
    "        'roc_auc': float(roc_auc),\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': classification_report(y_test_sample, y_pred, output_dict=True)\n",
    "    },\n",
    "    'test_size': len(test_sample),\n",
    "    'positive_rate': float(sum(y_pred) / len(y_pred)),\n",
    "    'true_positive_rate': float(sum([y_pred[i] * y_test_sample.iloc[i] for i in range(len(y_pred))]) / sum(y_test_sample)) if sum(y_test_sample) > 0 else 0\n",
    "}\n",
    "\n",
    "# Save test results to S3\n",
    "test_results_key = f'{prefix}/endpoints/{endpoint_name}/test_results.json'\n",
    "s3.Bucket(bucket).Object(test_results_key).put(\n",
    "    Body=json.dumps(test_results, indent=2, default=str)\n",
    ")\n",
    "print(f\"Saved test results to s3://{bucket}/{test_results_key}\")\n",
    "\n",
    "# Analyze a few sample transactions\n",
    "def analyze_transactions(X_sample, y_sample, predictor, model_feature_count=model_feature_count):\n",
    "    \"\"\"Analyze predictions for a sample of transactions\"\"\"\n",
    "    results = []\n",
    "    for i, (idx, row) in enumerate(X_sample.iterrows()):\n",
    "        features_str = format_features_for_prediction(row, feature_count=model_feature_count)\n",
    "        try:\n",
    "            response = predictor.predict(features_str)\n",
    "            pred_probability = float(response.decode('utf-8'))\n",
    "            is_fraud_prediction = pred_probability >= 0.5\n",
    "            \n",
    "            # Find the true label\n",
    "            true_label = y_sample.iloc[i]\n",
    "            \n",
    "            # Determine if prediction was correct\n",
    "            correct = (is_fraud_prediction == true_label)\n",
    "            \n",
    "            # Create a result dictionary\n",
    "            result = {\n",
    "                'index': idx,\n",
    "                'prediction': is_fraud_prediction,\n",
    "                'probability': pred_probability,\n",
    "                'true_label': true_label,\n",
    "                'correct': correct,\n",
    "                # Include a few key features for analysis\n",
    "                'features': {\n",
    "                    'amount': row.get('amount', 'N/A'),\n",
    "                    'is_vpn': row.get('is_vpn', 'N/A')\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing transaction {idx}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Get small samples of fraud and non-fraud cases\n",
    "fraud_indices = y_test_sample[y_test_sample == 1].index[:5]  # First 5 fraud cases\n",
    "non_fraud_indices = y_test_sample[y_test_sample == 0].index[:5]  # First 5 non-fraud cases\n",
    "\n",
    "fraud_sample = test_sample.loc[fraud_indices]\n",
    "non_fraud_sample = test_sample.loc[non_fraud_indices]\n",
    "\n",
    "# Analyze samples\n",
    "print(\"\\nAnalyzing sample transactions...\")\n",
    "if not fraud_sample.empty:\n",
    "    print(\"\\nSample Fraud Transactions Analysis:\")\n",
    "    fraud_analysis = analyze_transactions(fraud_sample, y_test_sample.loc[fraud_indices], predictor)\n",
    "    for result in fraud_analysis:\n",
    "        print(f\"Transaction {result['index']}: Predicted {'FRAUD' if result['prediction'] else 'OK'} \" \n",
    "            f\"(probability: {result['probability']:.4f}), True label: {'FRAUD' if result['true_label'] else 'OK'}, \"\n",
    "            f\"Correct: {result['correct']}\")\n",
    "        print(f\"  Amount: {result['features']['amount']}, VPN: {result['features']['is_vpn']}\")\n",
    "else:\n",
    "    print(\"No fraud samples in the test set subset\")\n",
    "\n",
    "if not non_fraud_sample.empty:\n",
    "    print(\"\\nSample Non-Fraud Transactions Analysis:\")\n",
    "    non_fraud_analysis = analyze_transactions(non_fraud_sample, y_test_sample.loc[non_fraud_indices], predictor)\n",
    "    for result in non_fraud_analysis:\n",
    "        print(f\"Transaction {result['index']}: Predicted {'FRAUD' if result['prediction'] else 'OK'} \" \n",
    "            f\"(probability: {result['probability']:.4f}), True label: {'FRAUD' if result['true_label'] else 'OK'}, \"\n",
    "            f\"Correct: {result['correct']}\")\n",
    "        print(f\"  Amount: {result['features']['amount']}, VPN: {result['features']['is_vpn']}\")\n",
    "else:\n",
    "    print(\"No non-fraud samples in the test set subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Model Evaluation:\")\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Create and display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Random prediction line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics for business context\n",
    "fraud_detected = cm[1, 1]  # True positives\n",
    "fraud_missed = cm[1, 0]    # False negatives\n",
    "false_alarms = cm[0, 1]    # False positives\n",
    "correct_negatives = cm[0, 0]  # True negatives\n",
    "\n",
    "# Calculate important business metrics\n",
    "detection_rate = fraud_detected / (fraud_detected + fraud_missed) if (fraud_detected + fraud_missed) > 0 else 0\n",
    "false_positive_rate = false_alarms / (false_alarms + correct_negatives) if (false_alarms + correct_negatives) > 0 else 0\n",
    "precision = fraud_detected / (fraud_detected + false_alarms) if (fraud_detected + false_alarms) > 0 else 0\n",
    "\n",
    "print(\"\\nBusiness Impact Metrics:\")\n",
    "print(f\"Fraud Detection Rate: {detection_rate:.2%}\")\n",
    "print(f\"False Alarm Rate: {false_positive_rate:.2%}\")\n",
    "print(f\"Precision (% of flagged transactions that are actual fraud): {precision:.2%}\")\n",
    "\n",
    "# Save detailed model details for reference\n",
    "model_details = {\n",
    "    'endpoint_name': endpoint_name,\n",
    "    'model_name': model_name,\n",
    "    'features': features,\n",
    "    'evaluation_timestamp': time.time(),\n",
    "    'performance': {\n",
    "        'roc_auc': float(roc_auc),\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
    "        'business_metrics': {\n",
    "            'fraud_detection_rate': float(detection_rate),\n",
    "            'false_alarm_rate': float(false_positive_rate),\n",
    "            'precision': float(precision),\n",
    "            'fraud_cases_detected': int(fraud_detected),\n",
    "            'fraud_cases_missed': int(fraud_missed),\n",
    "            'false_alarms': int(false_alarms)\n",
    "        }\n",
    "    },\n",
    "    'model_parameters': {\n",
    "        'threshold': 0.5,  # Default classification threshold\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to S3\n",
    "model_details_key = f'{prefix}/model-details.json'\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).Object(model_details_key).put(\n",
    "    Body=json.dumps(model_details, indent=2)\n",
    ")\n",
    "print(f\"Model details saved to s3://{bucket}/{model_details_key}\")\n",
    "\n",
    "# Calculate performance at different thresholds to find optimal threshold\n",
    "thresholds_to_try = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "threshold_metrics = []\n",
    "\n",
    "for threshold in thresholds_to_try:\n",
    "    # Convert probabilities to predictions using this threshold\n",
    "    y_pred_at_threshold = [1 if p >= threshold else 0 for p in y_pred_proba]\n",
    "    \n",
    "    # Calculate confusion matrix at this threshold\n",
    "    cm_at_threshold = confusion_matrix(y_test, y_pred_at_threshold)\n",
    "    \n",
    "    # Extract metrics\n",
    "    tp = cm_at_threshold[1, 1]  # True positives\n",
    "    fn = cm_at_threshold[1, 0]  # False negatives\n",
    "    fp = cm_at_threshold[0, 1]  # False positives\n",
    "    tn = cm_at_threshold[0, 0]  # True negatives\n",
    "    \n",
    "    # Calculate rates\n",
    "    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # Store metrics\n",
    "    threshold_metrics.append({\n",
    "        'threshold': threshold,\n",
    "        'detection_rate': detection_rate,\n",
    "        'false_alarm_rate': false_alarm_rate,\n",
    "        'precision': precision,\n",
    "        'true_positives': int(tp),\n",
    "        'false_negatives': int(fn),\n",
    "        'false_positives': int(fp),\n",
    "        'true_negatives': int(tn)\n",
    "    })\n",
    "\n",
    "# Plot metrics across different thresholds\n",
    "plt.figure(figsize=(10, 6))\n",
    "thresholds = [m['threshold'] for m in threshold_metrics]\n",
    "detection_rates = [m['detection_rate'] for m in threshold_metrics]\n",
    "false_alarm_rates = [m['false_alarm_rate'] for m in threshold_metrics]\n",
    "precisions = [m['precision'] for m in threshold_metrics]\n",
    "\n",
    "plt.plot(thresholds, detection_rates, 'bo-', label='Detection Rate')\n",
    "plt.plot(thresholds, false_alarm_rates, 'ro-', label='False Alarm Rate')\n",
    "plt.plot(thresholds, precisions, 'go-', label='Precision')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Performance Metrics at Different Thresholds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRecommended threshold based on business needs:\")\n",
    "# Find threshold with detection rate > 0.8 and lowest false alarm rate\n",
    "good_thresholds = [m for m in threshold_metrics if m['detection_rate'] >= 0.8]\n",
    "if good_thresholds:\n",
    "    recommended = min(good_thresholds, key=lambda x: x['false_alarm_rate'])\n",
    "    print(f\"Threshold: {recommended['threshold']}\")\n",
    "    print(f\"Detection Rate: {recommended['detection_rate']:.2%}\")\n",
    "    print(f\"False Alarm Rate: {recommended['false_alarm_rate']:.2%}\")\n",
    "    print(f\"Precision: {recommended['precision']:.2%}\")\n",
    "else:\n",
    "    print(\"No threshold meets minimum detection rate requirement of 80%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "def generate_test_transactions(num_transactions=10, fraud_ratio=0.3):\n",
    "\n",
    "    test_transactions = []\n",
    "    \n",
    "    # Calculate counts\n",
    "    num_fraud = int(num_transactions * fraud_ratio)\n",
    "    num_legitimate = num_transactions - num_fraud\n",
    "    \n",
    "    # Generate legitimate transactions\n",
    "    for _ in range(num_legitimate):\n",
    "        transaction = generate_transaction(is_fraud=False)\n",
    "        test_transactions.append(transaction)\n",
    "    \n",
    "    # Generate fraudulent transactions\n",
    "    for _ in range(num_fraud):\n",
    "        transaction = generate_transaction(is_fraud=True)\n",
    "        test_transactions.append(transaction)\n",
    "    \n",
    "    # Shuffle to mix fraud and legitimate\n",
    "    random.shuffle(test_transactions)\n",
    "    \n",
    "    return test_transactions\n",
    "\n",
    "def generate_transaction(is_fraud=False):\n",
    "    \"\"\"Generate a single transaction with realistic properties\"\"\"\n",
    "    # Generate a transaction ID\n",
    "    transaction_id = f\"T{uuid.uuid4().hex[:8].upper()}\"\n",
    "    \n",
    "    # Generate a user ID\n",
    "    user_id = f\"U{random.randint(10000, 99999)}\"\n",
    "    \n",
    "    # Generate timestamp (current time with small random offset)\n",
    "    timestamp = datetime.datetime.now() - datetime.timedelta(\n",
    "        minutes=random.randint(0, 59),\n",
    "        seconds=random.randint(0, 59)\n",
    "    )\n",
    "    timestamp_str = timestamp.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    # Generate amount based on fraud flag\n",
    "    if is_fraud:\n",
    "        amount = round(random.uniform(800, 3000), 2) if random.random() < 0.7 else round(random.uniform(0.5, 20), 2)\n",
    "    else:\n",
    "        amount = round(random.uniform(50, 500), 2)\n",
    "    \n",
    "    # Generate device type\n",
    "    device_options = ['mobile', 'desktop', 'tablet']\n",
    "    if is_fraud:\n",
    "        device_type = random.choices(device_options, weights=[0.7, 0.2, 0.1])[0]\n",
    "    else:\n",
    "        device_type = random.choices(device_options, weights=[0.4, 0.5, 0.1])[0]\n",
    "    \n",
    "    # Generate location\n",
    "    locations = [\n",
    "        'California, USA', 'New York, USA', 'Texas, USA', 'Florida, USA', \n",
    "        'Illinois, USA', 'London, UK', 'Paris, France', 'Berlin, Germany', \n",
    "        'Tokyo, Japan', 'Sydney, Australia'\n",
    "    ]\n",
    "    \n",
    "    if is_fraud:\n",
    "        location = random.choice(locations[5:])  # Foreign locations\n",
    "    else:\n",
    "        location = random.choice(locations[:5])  # US locations\n",
    "    \n",
    "    # Generate VPN usage\n",
    "    if is_fraud:\n",
    "        is_vpn = random.choices([True, False], weights=[0.7, 0.3])[0]\n",
    "    else:\n",
    "        is_vpn = random.choices([True, False], weights=[0.1, 0.9])[0]\n",
    "    \n",
    "    # Generate card type\n",
    "    card_options = ['credit', 'debit', 'gift']\n",
    "    if is_fraud:\n",
    "        card_type = random.choices(card_options, weights=[0.5, 0.2, 0.3])[0]\n",
    "    else:\n",
    "        card_type = random.choices(card_options, weights=[0.4, 0.55, 0.05])[0]\n",
    "    \n",
    "    # Generate status\n",
    "    status_options = ['approved', 'pending', 'declined']\n",
    "    if is_fraud and random.random() < 0.3:\n",
    "        status = random.choices(status_options, weights=[0.6, 0.2, 0.2])[0]\n",
    "    else:\n",
    "        status = random.choices(status_options, weights=[0.95, 0.03, 0.02])[0]\n",
    "    \n",
    "    # Create transaction\n",
    "    return {\n",
    "        'transaction_id': transaction_id,\n",
    "        'user_id': user_id,\n",
    "        'timestamp': timestamp_str,\n",
    "        'amount': amount,\n",
    "        'device_type': device_type,\n",
    "        'location': location,\n",
    "        'is_vpn': is_vpn,\n",
    "        'card_type': card_type,\n",
    "        'status': status\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    transactions = generate_test_transactions(20, fraud_ratio=0.3)\n",
    "    print(f\"Generated {len(transactions)} transactions\")\n",
    "    print(f\"Sample transaction: {transactions[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class TransactionFeatureExtractor:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, s3_bucket=None, feature_info_key=None, dynamodb_table=None):\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.dynamodb = boto3.resource('dynamodb')\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.feature_info_key = feature_info_key\n",
    "        self.dynamodb_table = dynamodb_table\n",
    "        \n",
    "        # Default feature lists in case we can't load from S3\n",
    "        self.feature_names = []\n",
    "        self.categorical_features = []\n",
    "        self.numeric_features = []\n",
    "        \n",
    "        # Load feature info if provided\n",
    "        if s3_bucket and feature_info_key:\n",
    "            self.load_feature_info()\n",
    "        \n",
    "        # Cache for location and device fraud rates\n",
    "        self.location_fraud_rates = {}\n",
    "        self.device_fraud_rates = {}\n",
    "        self.card_fraud_rates = {}\n",
    "        \n",
    "        # Load fraud rates if possible\n",
    "        self.load_fraud_rates()\n",
    "    \n",
    "    def load_feature_info(self):\n",
    "        \"\"\"Load feature information from S3\"\"\"\n",
    "        try:\n",
    "            response = self.s3_client.get_object(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Key=self.feature_info_key\n",
    "            )\n",
    "            feature_info = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            \n",
    "            self.feature_names = feature_info.get('feature_names', [])\n",
    "            self.categorical_features = feature_info.get('categorical_features', [])\n",
    "            self.numeric_features = feature_info.get('numeric_features', [])\n",
    "            \n",
    "            logger.info(f\"Loaded feature info with {len(self.feature_names)} features\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading feature info: {str(e)}\")\n",
    "    \n",
    "    def load_fraud_rates(self):\n",
    "        \"\"\"Load fraud rates for categorical features if available\"\"\"\n",
    "        try:\n",
    "            fraud_rates_key = 'models/fraud_rates.json'\n",
    "            response = self.s3_client.get_object(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Key=fraud_rates_key\n",
    "            )\n",
    "            fraud_rates = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            \n",
    "            self.location_fraud_rates = fraud_rates.get('location', {})\n",
    "            self.device_fraud_rates = fraud_rates.get('device_type', {})\n",
    "            self.card_fraud_rates = fraud_rates.get('card_type', {})\n",
    "            \n",
    "            logger.info(\"Loaded fraud rates for categorical features\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load fraud rates, using defaults: {str(e)}\")\n",
    "            # Set default rates based on historical data analysis\n",
    "            self.location_fraud_rates = {\n",
    "                'California, USA': 0.02, 'New York, USA': 0.02, 'Texas, USA': 0.02,\n",
    "                'Florida, USA': 0.03, 'Illinois, USA': 0.02, 'London, UK': 0.03,\n",
    "                'Paris, France': 0.04, 'Berlin, Germany': 0.04, 'Tokyo, Japan': 0.05,\n",
    "                'Sydney, Australia': 0.05, 'Unknown': 0.09\n",
    "            }\n",
    "            self.device_fraud_rates = {'mobile': 0.03, 'desktop': 0.01, 'tablet': 0.02}\n",
    "            self.card_fraud_rates = {'credit': 0.025, 'debit': 0.01, 'gift': 0.05}\n",
    "    \n",
    "    def get_user_stats(self, user_id):\n",
    "        \"\"\"\n",
    "        Get user transaction statistics from DynamoDB.\n",
    "        In production, this would query a database of user history.\n",
    "        \n",
    "        Returns a dictionary of user stats or None if not found\n",
    "        \"\"\"\n",
    "        if not self.dynamodb_table:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            table = self.dynamodb.Table(self.dynamodb_table)\n",
    "            response = table.get_item(Key={'user_id': user_id})\n",
    "            \n",
    "            if 'Item' in response:\n",
    "                return response['Item'].get('stats', {})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving user stats: {str(e)}\")\n",
    "        \n",
    "        # Return default values if no data found or error\n",
    "        return {\n",
    "            'transaction_count': 1,\n",
    "            'avg_amount': 0,\n",
    "            'std_amount': 0,\n",
    "            'max_amount': 0,\n",
    "            'vpn_ratio': 0\n",
    "        }\n",
    "    \n",
    "    def extract_features(self, transaction):\n",
    "        # Create a dictionary to hold all features\n",
    "        features_dict = {}\n",
    "        \n",
    "        # Basic transaction features\n",
    "        features_dict['amount'] = float(transaction.get('amount', 0))\n",
    "        features_dict['is_vpn'] = 1 if transaction.get('is_vpn', False) else 0\n",
    "        \n",
    "        # Time-based features\n",
    "        try:\n",
    "            if isinstance(transaction.get('timestamp'), str):\n",
    "                timestamp = datetime.datetime.strptime(\n",
    "                    transaction.get('timestamp'),\n",
    "                    '%Y-%m-%dT%H:%M:%SZ'\n",
    "                )\n",
    "            else:\n",
    "                timestamp = transaction.get('timestamp', datetime.datetime.now())\n",
    "                \n",
    "            features_dict['hour_of_day'] = timestamp.hour\n",
    "            features_dict['day_of_week'] = timestamp.weekday()\n",
    "            features_dict['month'] = timestamp.month\n",
    "            features_dict['day_of_month'] = timestamp.day\n",
    "            features_dict['is_weekend'] = 1 if timestamp.weekday() >= 5 else 0\n",
    "            features_dict['is_night'] = 1 if (timestamp.hour < 6 or timestamp.hour >= 22) else 0\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing timestamp: {str(e)}\")\n",
    "            features_dict['hour_of_day'] = 0\n",
    "            features_dict['day_of_week'] = 0\n",
    "            features_dict['month'] = 1\n",
    "            features_dict['day_of_month'] = 1\n",
    "            features_dict['is_weekend'] = 0\n",
    "            features_dict['is_night'] = 0\n",
    "        \n",
    "        # Get user statistics for behavioral features\n",
    "        user_id = transaction.get('user_id')\n",
    "        user_stats = self.get_user_stats(user_id)\n",
    "        \n",
    "        if user_stats:\n",
    "            # User transaction count\n",
    "            features_dict['user_transaction_count'] = user_stats.get('transaction_count', 1)\n",
    "            \n",
    "            # User average amount\n",
    "            user_avg_amount = user_stats.get('avg_amount', 0)\n",
    "            features_dict['user_avg_amount'] = user_avg_amount\n",
    "            \n",
    "            # User amount standard deviation\n",
    "            user_amount_std = user_stats.get('std_amount', 1)\n",
    "            features_dict['user_amount_std'] = user_amount_std\n",
    "            \n",
    "            # User min and max amount\n",
    "            features_dict['user_min_amount'] = user_stats.get('min_amount', 0)\n",
    "            features_dict['user_max_amount'] = user_stats.get('max_amount', 0)\n",
    "            \n",
    "            # VPN usage ratio\n",
    "            features_dict['user_vpn_ratio'] = user_stats.get('vpn_ratio', 0)\n",
    "            \n",
    "            # Calculate z-score\n",
    "            if user_amount_std > 0:\n",
    "                features_dict['amount_zscore'] = (features_dict['amount'] - user_avg_amount) / user_amount_std\n",
    "            else:\n",
    "                features_dict['amount_zscore'] = 0\n",
    "                \n",
    "            # Calculate amount to max ratio\n",
    "            max_amount = user_stats.get('max_amount', 0)\n",
    "            if max_amount > 0:\n",
    "                features_dict['amount_to_max_ratio'] = features_dict['amount'] / max_amount\n",
    "            else:\n",
    "                features_dict['amount_to_max_ratio'] = 1\n",
    "                \n",
    "            # Is this an unusual amount for the user?\n",
    "            is_unusual = 0\n",
    "            if user_amount_std > 0 and abs(features_dict['amount_zscore']) > 2:\n",
    "                is_unusual = 1\n",
    "            features_dict['is_unusual_amount'] = is_unusual\n",
    "            \n",
    "            # Is this a new user?\n",
    "            features_dict['is_new_user'] = 1 if features_dict['user_transaction_count'] <= 3 else 0\n",
    "            \n",
    "            # Is this the user's largest transaction?\n",
    "            features_dict['is_largest_tx'] = 1 if features_dict['amount'] >= max_amount * 0.95 else 0\n",
    "        else:\n",
    "            # Default values for new users or if user data not available\n",
    "            features_dict['user_transaction_count'] = 1\n",
    "            features_dict['user_avg_amount'] = 0\n",
    "            features_dict['user_amount_std'] = 1\n",
    "            features_dict['user_min_amount'] = 0\n",
    "            features_dict['user_max_amount'] = 0\n",
    "            features_dict['user_vpn_ratio'] = 0\n",
    "            features_dict['amount_zscore'] = 0\n",
    "            features_dict['amount_to_max_ratio'] = 1\n",
    "            features_dict['is_unusual_amount'] = 0\n",
    "            features_dict['is_new_user'] = 1\n",
    "            features_dict['is_largest_tx'] = 1\n",
    "        \n",
    "        # Location risk score based on historical fraud rates\n",
    "        location = transaction.get('location', 'Unknown')\n",
    "        features_dict['location_fraud_rate'] = self.location_fraud_rates.get(location, 0.05)\n",
    "        \n",
    "        # Device fraud rate\n",
    "        device_type = transaction.get('device_type', 'unknown')\n",
    "        features_dict['device_fraud_rate'] = self.device_fraud_rates.get(device_type, 0.02)\n",
    "        \n",
    "        # Card fraud rate\n",
    "        card_type = transaction.get('card_type', 'unknown')\n",
    "        features_dict['card_fraud_rate'] = self.card_fraud_rates.get(card_type, 0.02)\n",
    "        \n",
    "        # Create one-hot encoded features for categorical variables\n",
    "        # Location one-hot encoding\n",
    "        for loc in self.location_fraud_rates.keys():\n",
    "            features_dict[f'location_{loc.replace(\", \", \"_\").lower()}'] = 1 if location == loc else 0\n",
    "            \n",
    "        # Device type one-hot encoding\n",
    "        for dev in ['mobile', 'desktop', 'tablet']:\n",
    "            features_dict[f'device_type_{dev}'] = 1 if device_type == dev else 0\n",
    "            \n",
    "        # Card type one-hot encoding\n",
    "        for card in ['credit', 'debit', 'gift']:\n",
    "            features_dict[f'card_type_{card}'] = 1 if card_type == card else 0\n",
    "            \n",
    "        # Transaction status one-hot encoding\n",
    "        status = transaction.get('status', 'approved')\n",
    "        for st in ['approved', 'pending', 'declined']:\n",
    "            features_dict[f'status_{st}'] = 1 if status == st else 0\n",
    "        \n",
    "        # Assemble feature string in the correct order based on feature names\n",
    "        feature_values = []\n",
    "        for feature in self.feature_names:\n",
    "            if feature in features_dict:\n",
    "                feature_values.append(str(features_dict[feature]))\n",
    "            else:\n",
    "                feature_values.append('0')  # Default value for missing features\n",
    "        \n",
    "        return ','.join(feature_values)\n",
    "    \n",
    "    def format_batch_for_prediction(self, transactions):\n",
    "        return [self.extract_features(tx) for tx in transactions]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def process_transactions_for_prediction(transactions, model_bucket=None):\n",
    "    # Initialize feature extractor\n",
    "    extractor = TransactionFeatureExtractor(\n",
    "        s3_bucket=model_bucket or os.environ.get('MODEL_DATA_S3_BUCKET', 'ssense-fraud-model-data'),\n",
    "        feature_info_key='fraud-classifier/model/feature_info.json',\n",
    "        dynamodb_table=os.environ.get('TRANSACTION_HISTORY_TABLE', 'ssense-fraud-user-history')\n",
    "    )\n",
    "    \n",
    "    # Process transactions\n",
    "    return extractor.format_batch_for_prediction(transactions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinesis Integration for Real-time Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_transactions_to_kinesis(transactions, stream_name=config.KINESIS_STREAM_NAME):\n",
    "    \"\"\"Send transactions to Kinesis for real-time processing\"\"\"\n",
    "    kinesis = boto3.client('kinesis')\n",
    "    \n",
    "    for transaction in transactions:\n",
    "        # Convert transaction to JSON\n",
    "        transaction_json = json.dumps(transaction)\n",
    "        \n",
    "        # Send to Kinesis\n",
    "        response = kinesis.put_record(\n",
    "            StreamName=stream_name,\n",
    "            Data=transaction_json,\n",
    "            PartitionKey=transaction['transaction_id']\n",
    "        )\n",
    "        \n",
    "        print(f\"Sent transaction {transaction['transaction_id']} to Kinesis: Shard {response['ShardId']}\")\n",
    "        \n",
    "        # Small delay to avoid throttling\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Generate a batch of transactions and send to Kinesis\n",
    "kinesis_test_transactions = generate_test_transactions(10, fraud_ratio=0.2)\n",
    "send_transactions_to_kinesis(kinesis_test_transactions)\n",
    "\n",
    "print(f\"Sent {len(kinesis_test_transactions)} transactions to Kinesis stream {config.KINESIS_STREAM_NAME}\")\n",
    "print(\"These will be processed by the Lambda function and results stored in DynamoDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard for Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dynamodb_for_results(transaction_ids, table_name=config.DYNAMODB_TABLE):\n",
    "    \"\"\"Check DynamoDB for processing results of test transactions\"\"\"\n",
    "    dynamodb = boto3.resource('dynamodb')\n",
    "    table = dynamodb.Table(table_name)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for tx_id in transaction_ids:\n",
    "        try:\n",
    "            response = table.get_item(Key={'transaction_id': tx_id})\n",
    "            if 'Item' in response:\n",
    "                results.append(response['Item'])\n",
    "            else:\n",
    "                print(f\"Transaction {tx_id} not found in DynamoDB yet.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving transaction {tx_id}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Give Lambda some time to process\n",
    "print(\"Waiting for Lambda to process transactions...\")\n",
    "time.sleep(10)  # Wait 10 seconds\n",
    "\n",
    "# Check DynamoDB for results\n",
    "tx_ids = [tx['transaction_id'] for tx in kinesis_test_transactions]\n",
    "results = check_dynamodb_for_results(tx_ids)\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"Found {len(results)} transactions in DynamoDB\")\n",
    "    \n",
    "    # Plot fraud probabilities\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='transaction_id', y='fraud_probability', data=results_df)\n",
    "    plt.title('Fraud Probabilities for Test Transactions')\n",
    "    plt.xlabel('Transaction ID')\n",
    "    plt.ylabel('Fraud Probability')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results found in DynamoDB yet. Lambda might still be processing or there might be an issue.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
